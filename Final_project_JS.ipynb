{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark_dist_explore\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/c2/2f19468300f7c9d25dc526f0c11779d87e1a7c07d999347cf71a13282c0b/pyspark_dist_explore-0.1.4-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: numpy in /opt/anaconda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.15.0)\n",
      "Requirement not upgraded as not directly required: matplotlib in /opt/anaconda/lib/python3.6/site-packages (from pyspark_dist_explore) (2.2.2)\n",
      "Requirement not upgraded as not directly required: pandas in /opt/anaconda/lib/python3.6/site-packages (from pyspark_dist_explore) (0.23.3)\n",
      "Requirement not upgraded as not directly required: scipy in /opt/anaconda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.1.0)\n",
      "Requirement not upgraded as not directly required: cycler>=0.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (0.10.0)\n",
      "Requirement not upgraded as not directly required: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2.2.0)\n",
      "Requirement not upgraded as not directly required: python-dateutil>=2.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2.7.3)\n",
      "Requirement not upgraded as not directly required: pytz in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2018.5)\n",
      "Requirement not upgraded as not directly required: six>=1.10 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (1.11.0)\n",
      "Requirement not upgraded as not directly required: kiwisolver>=1.0.1 in /opt/anaconda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (1.0.1)\n",
      "Requirement not upgraded as not directly required: setuptools in /opt/anaconda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->pyspark_dist_explore) (39.2.0)\n",
      "\u001b[31mdistributed 1.22.0 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: pyspark-dist-explore\n",
      "Successfully installed pyspark-dist-explore-0.1.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyspark_dist_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display, HTML, display_html #usefull to display wide tables\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram\n",
    "from pyspark.sql import functions as F, types, SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifty_lines = ['0\\t1\\t1\\t5\\t0\\t1382\\t4\\t15\\t2\\t181\\t1\\t2\\t\\t2\\t68fd1e64\\t80e26c9b\\tfb936136\\t7b4723c4\\t25c83c98\\t7e0ccccf\\tde7995b8\\t1f89b562\\ta73ee510\\ta8cd5504\\tb2cb9c98\\t37c9c164\\t2824a5f6\\t1adce6ef\\t8ba8b39a\\t891b62e7\\te5ba7672\\tf54016b9\\t21ddcdc9\\tb1252a9d\\t07b5194c\\t\\t3a171ecb\\tc5c50484\\te8b83407\\t9727dd16',\n",
    " '0\\t2\\t0\\t44\\t1\\t102\\t8\\t2\\t2\\t4\\t1\\t1\\t\\t4\\t68fd1e64\\tf0cf0024\\t6f67f7e5\\t41274cd7\\t25c83c98\\tfe6b92e5\\t922afcc0\\t0b153874\\ta73ee510\\t2b53e5fb\\t4f1b46f3\\t623049e6\\td7020589\\tb28479f6\\te6c5b5cd\\tc92f3b61\\t07c540c4\\tb04e4670\\t21ddcdc9\\t5840adea\\t60f6221e\\t\\t3a171ecb\\t43f13e8b\\te8b83407\\t731c3655',\n",
    " '0\\t2\\t0\\t1\\t14\\t767\\t89\\t4\\t2\\t245\\t1\\t3\\t3\\t45\\t287e684f\\t0a519c5c\\t02cf9876\\tc18be181\\t25c83c98\\t7e0ccccf\\tc78204a1\\t0b153874\\ta73ee510\\t3b08e48b\\t5f5e6091\\t8fe001f4\\taa655a2f\\t07d13a8f\\t6dc710ed\\t36103458\\t8efede7f\\t3412118d\\t\\t\\te587c466\\tad3062eb\\t3a171ecb\\t3b183c5c\\t\\t',\n",
    " '0\\t\\t893\\t\\t\\t4392\\t\\t0\\t0\\t0\\t\\t0\\t\\t\\t68fd1e64\\t2c16a946\\ta9a87e68\\t2e17d6f6\\t25c83c98\\tfe6b92e5\\t2e8a689b\\t0b153874\\ta73ee510\\tefea433b\\te51ddf94\\ta30567ca\\t3516f6e6\\t07d13a8f\\t18231224\\t52b8680f\\t1e88c74f\\t74ef3502\\t\\t\\t6b3a5ca6\\t\\t3a171ecb\\t9117a34a\\t\\t',\n",
    " '0\\t3\\t-1\\t\\t0\\t2\\t0\\t3\\t0\\t0\\t1\\t1\\t\\t0\\t8cf07265\\tae46a29d\\tc81688bb\\tf922efad\\t25c83c98\\t13718bbd\\tad9fa255\\t0b153874\\ta73ee510\\t5282c137\\te5d8af57\\t66a76a26\\tf06c53ac\\t1adce6ef\\t8ff4b403\\t01adbab4\\t1e88c74f\\t26b3c7a7\\t\\t\\t21c9516a\\t\\t32c7478e\\tb34f3128\\t\\t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = \"hw5_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdd = sc.parallelize(fifty_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t1\\t1\\t5\\t0\\t1382\\t4\\t15\\t2\\t181\\t1\\t2\\t\\t2\\t68fd1e64\\t80e26c9b\\tfb936136\\t7b4723c4\\t25c83c98\\t7e0ccccf\\tde7995b8\\t1f89b562\\ta73ee510\\ta8cd5504\\tb2cb9c98\\t37c9c164\\t2824a5f6\\t1adce6ef\\t8ba8b39a\\t891b62e7\\te5ba7672\\tf54016b9\\t21ddcdc9\\tb1252a9d\\t07b5194c\\t\\t3a171ecb\\tc5c50484\\te8b83407\\t9727dd16',\n",
       " '0\\t2\\t0\\t44\\t1\\t102\\t8\\t2\\t2\\t4\\t1\\t1\\t\\t4\\t68fd1e64\\tf0cf0024\\t6f67f7e5\\t41274cd7\\t25c83c98\\tfe6b92e5\\t922afcc0\\t0b153874\\ta73ee510\\t2b53e5fb\\t4f1b46f3\\t623049e6\\td7020589\\tb28479f6\\te6c5b5cd\\tc92f3b61\\t07c540c4\\tb04e4670\\t21ddcdc9\\t5840adea\\t60f6221e\\t\\t3a171ecb\\t43f13e8b\\te8b83407\\t731c3655',\n",
       " '0\\t2\\t0\\t1\\t14\\t767\\t89\\t4\\t2\\t245\\t1\\t3\\t3\\t45\\t287e684f\\t0a519c5c\\t02cf9876\\tc18be181\\t25c83c98\\t7e0ccccf\\tc78204a1\\t0b153874\\ta73ee510\\t3b08e48b\\t5f5e6091\\t8fe001f4\\taa655a2f\\t07d13a8f\\t6dc710ed\\t36103458\\t8efede7f\\t3412118d\\t\\t\\te587c466\\tad3062eb\\t3a171ecb\\t3b183c5c\\t\\t',\n",
       " '0\\t\\t893\\t\\t\\t4392\\t\\t0\\t0\\t0\\t\\t0\\t\\t\\t68fd1e64\\t2c16a946\\ta9a87e68\\t2e17d6f6\\t25c83c98\\tfe6b92e5\\t2e8a689b\\t0b153874\\ta73ee510\\tefea433b\\te51ddf94\\ta30567ca\\t3516f6e6\\t07d13a8f\\t18231224\\t52b8680f\\t1e88c74f\\t74ef3502\\t\\t\\t6b3a5ca6\\t\\t3a171ecb\\t9117a34a\\t\\t',\n",
       " '0\\t3\\t-1\\t\\t0\\t2\\t0\\t3\\t0\\t0\\t1\\t1\\t\\t0\\t8cf07265\\tae46a29d\\tc81688bb\\tf922efad\\t25c83c98\\t13718bbd\\tad9fa255\\t0b153874\\ta73ee510\\t5282c137\\te5d8af57\\t66a76a26\\tf06c53ac\\t1adce6ef\\t8ff4b403\\t01adbab4\\t1e88c74f\\t26b3c7a7\\t\\t\\t21c9516a\\t\\t32c7478e\\tb34f3128\\t\\t']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t1\\t1\\t5\\t0\\t1382\\t4\\t15\\t2\\t181\\t1\\t2\\t\\t2\\t68fd1e64\\t80e26c9b\\tfb936136\\t7b4723c4\\t25c83c98\\t7e0ccccf\\tde7995b8\\t1f89b562\\ta73ee510\\ta8cd5504\\tb2cb9c98\\t37c9c164\\t2824a5f6\\t1adce6ef\\t8ba8b39a\\t891b62e7\\te5ba7672\\tf54016b9\\t21ddcdc9\\tb1252a9d\\t07b5194c\\t\\t3a171ecb\\tc5c50484\\te8b83407\\t9727dd16']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0\\t1\\t1\\t5\\t0\\t1382\\t4\\t15\\t2\\t181\\t1\\t2\\t\\t2\\t68fd1e64\\t80e26c9b\\tfb936136\\t7b4723c4\\t25c83c98\\t7e0ccccf\\tde7995b8\\t1f89b562\\ta73ee510\\ta8cd5504\\tb2cb9c98\\t37c9c164\\t2824a5f6\\t1adce6ef\\t8ba8b39a\\t891b62e7\\te5ba7672\\tf54016b9\\t21ddcdc9\\tb1252a9d\\t07b5194c\\t\\t3a171ecb\\tc5c50484\\te8b83407\\t9727dd16',\n",
       " '0\\t2\\t0\\t44\\t1\\t102\\t8\\t2\\t2\\t4\\t1\\t1\\t\\t4\\t68fd1e64\\tf0cf0024\\t6f67f7e5\\t41274cd7\\t25c83c98\\tfe6b92e5\\t922afcc0\\t0b153874\\ta73ee510\\t2b53e5fb\\t4f1b46f3\\t623049e6\\td7020589\\tb28479f6\\te6c5b5cd\\tc92f3b61\\t07c540c4\\tb04e4670\\t21ddcdc9\\t5840adea\\t60f6221e\\t\\t3a171ecb\\t43f13e8b\\te8b83407\\t731c3655']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_dict = {\n",
    "   \"Andrew\" : [1, 4, 8, 2.9, 1, \"Computer Science\", \"History\"],\n",
    "   \"Vikram\" : [0, 3, 9, 3.5, 0, \"Math\", \"French\"],\n",
    "   \"Samantha\" : [0, 2, 9, 3.4, 1, \"Math\", \"Computer Science\"],\n",
    "   \"Chris\" : [0, 2, 8, 3.7, 0, \"Data Science\", \"English\"],\n",
    "   \"Brenda\" : [0, 3, 7, 3.9, 0, \"Data Science\", \"Math\"],\n",
    "   \"Doug\" : [1, 3, 7, 3.2, 0, \"English\", \"Data Science\"],\n",
    "   \"Maria\" : [1, 4, 8, 2.9, 1, \"Computer Science\", \"History\"],\n",
    "   \"Tony\" : [0, 2, 7, 2.8, 1, \"Data Science\", \"French\"],\n",
    "   \"Tonya\" : [1, 4, 9, 3.7, 0, \"Computer Science\", \"German\"],\n",
    "   \"Ruth\" : [0, 3, 6, 3.1, 1, \"Astrophysics\", \"Math\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,4,8,2.9,1,Computer Science,History\n",
      "0,3,9,3.5,0,Math,French\n",
      "0,2,9,3.4,1,Math,Computer Science\n",
      "0,2,8,3.7,0,Data Science,English\n",
      "0,3,7,3.9,0,Data Science,Math\n",
      "1,3,7,3.2,0,English,Data Science\n",
      "1,4,8,2.9,1,Computer Science,History\n",
      "0,2,7,2.8,1,Data Science,French\n",
      "1,4,9,3.7,0,Computer Science,German\n",
      "0,3,6,3.1,1,Astrophysics,Math\n"
     ]
    }
   ],
   "source": [
    "for x in students_dict.values():\n",
    "    print(','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD = sc.parallelize(students_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD1 = sc.parallelize([ ','.join(map(str, x)) for x in students_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyDF = toyRDD.map(lambda x: x).toDF([\"dropout\", \"int_1\", \"int_2\", \"int_3\", \"int_4\", \"categ_1\", \"categ_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyDF = toyDF.withColumn(\"int_1\", toyDF[\"int_1\"].cast(types.IntegerType()))\n",
    "toyDF = toyDF.withColumn(\"int_2\", toyDF[\"int_2\"].cast(types.IntegerType()))\n",
    "toyDF = toyDF.withColumn(\"int_3\", toyDF[\"int_3\"].cast(types.IntegerType()))\n",
    "toyDF = toyDF.withColumn(\"int_4\", toyDF[\"int_4\"].cast(types.IntegerType()))\n",
    "toyDF = toyDF.withColumn(\"dropout\", toyDF[\"int_4\"].cast(types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dropout=1, int_1=4, int_2=8, int_3=2, int_4=1, categ_1='Computer Science', categ_2='History')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Data for manual OHE\n",
    "# Note: the first data point does not include any value for the optional third feature\n",
    "sample_one = [(0, 'mouse'), (1, 'black')]\n",
    "sample_two = [(0, 'cat'), (1, 'tabby'), (2, 'mouse')]\n",
    "sample_three =  [(0, 'bear'), (1, 'black'), (2, 'salmon')]\n",
    "\n",
    "def sample_to_row(sample):\n",
    "    tmp_dict = defaultdict(lambda: None)\n",
    "    tmp_dict.update(sample)\n",
    "    return [tmp_dict[i] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+----------------+----------------+\n",
      "|dropout|int_1|int_2|int_3|int_4|         categ_1|         categ_2|\n",
      "+-------+-----+-----+-----+-----+----------------+----------------+\n",
      "|      1|    4|    8|    2|    1|Computer Science|         History|\n",
      "|      0|    3|    9|    3|    0|            Math|          French|\n",
      "|      1|    2|    9|    3|    1|            Math|Computer Science|\n",
      "|      0|    2|    8|    3|    0|    Data Science|         English|\n",
      "|      0|    3|    7|    3|    0|    Data Science|            Math|\n",
      "|      0|    3|    7|    3|    0|         English|    Data Science|\n",
      "|      1|    4|    8|    2|    1|Computer Science|         History|\n",
      "|      1|    2|    7|    2|    1|    Data Science|          French|\n",
      "|      0|    4|    9|    3|    0|Computer Science|          German|\n",
      "|      1|    3|    6|    3|    1|    Astrophysics|            Math|\n",
      "+-------+-----+-----+-----+-----+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toyDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing toy.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile toy.txt\n",
    "1,4,8,2.9,1,Computer Science,History\n",
    "0,3,9,3.5,0,Math,French\n",
    "0,2,9,3.4,1,Math,Computer Science\n",
    "0,2,8,3.7,0,Data Science,English\n",
    "0,3,7,3.9,0,Data Science,Math\n",
    "1,3,7,3.2,0,English,Data Science\n",
    "1,4,8,2.9,1,Computer Science,History\n",
    "0,2,7,2.8,1,Data Science,French\n",
    "1,4,9,3.7,0,Computer Science,German\n",
    "0,3,6,3.1,1,Astrophysics,Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mouse', 'black', None, None]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_to_row(sample_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c02fe212f885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoyDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categ_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "toyDF[\"categ_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d84b8e77c204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoyRDD1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "raw_df = sqlContext.createDataFrame(toyRDD1).withColumnRenamed(\"value\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----------------+----------------+\n",
      "| _1| _2| _3| _4| _5|              _6|              _7|\n",
      "+---+---+---+---+---+----------------+----------------+\n",
      "|  1|  4|  8|2.9|  1|Computer Science|         History|\n",
      "|  0|  3|  9|3.5|  0|            Math|          French|\n",
      "|  0|  2|  9|3.4|  1|            Math|Computer Science|\n",
      "|  0|  2|  8|3.7|  0|    Data Science|         English|\n",
      "|  0|  3|  7|3.9|  0|    Data Science|            Math|\n",
      "|  1|  3|  7|3.2|  0|         English|    Data Science|\n",
      "|  1|  4|  8|2.9|  1|Computer Science|         History|\n",
      "|  0|  2|  7|2.8|  1|    Data Science|          French|\n",
      "|  1|  4|  9|3.7|  0|Computer Science|          German|\n",
      "|  0|  3|  6|3.1|  1|    Astrophysics|            Math|\n",
      "+---+---+---+---+---+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,4,8,2.9,1,Computer Science,History',\n",
       " '0,3,9,3.5,0,Math,French',\n",
       " '0,2,9,3.4,1,Math,Computer Science',\n",
       " '0,2,8,3.7,0,Data Science,English',\n",
       " '0,3,7,3.9,0,Data Science,Math',\n",
       " '1,3,7,3.2,0,English,Data Science',\n",
       " '1,4,8,2.9,1,Computer Science,History',\n",
       " '0,2,7,2.8,1,Data Science,French',\n",
       " '1,4,9,3.7,0,Computer Science,German',\n",
       " '0,3,6,3.1,1,Astrophysics,Math']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyRDD1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(students_dict.values()), columns=[\"dropout\", \"int_1\", \"int_2\", \"int_3\", \"int_4\", \"categ_1\", \"categ_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout</th>\n",
       "      <th>int_1</th>\n",
       "      <th>int_2</th>\n",
       "      <th>int_3</th>\n",
       "      <th>int_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.516398</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1.032796</td>\n",
       "      <td>0.387155</td>\n",
       "      <td>0.527046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dropout      int_1      int_2      int_3      int_4\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.400000   3.000000   7.800000   3.310000   0.500000\n",
       "std     0.516398   0.816497   1.032796   0.387155   0.527046\n",
       "min     0.000000   2.000000   6.000000   2.800000   0.000000\n",
       "25%     0.000000   2.250000   7.000000   2.950000   0.000000\n",
       "50%     0.000000   3.000000   8.000000   3.300000   0.500000\n",
       "75%     1.000000   3.750000   8.750000   3.650000   1.000000\n",
       "max     1.000000   4.000000   9.000000   3.900000   1.000000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout</th>\n",
       "      <th>int_1</th>\n",
       "      <th>int_2</th>\n",
       "      <th>int_3</th>\n",
       "      <th>int_4</th>\n",
       "      <th>categ_1</th>\n",
       "      <th>categ_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>Math</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>Math</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Astrophysics</td>\n",
       "      <td>Math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dropout  int_1  int_2  int_3  int_4           categ_1           categ_2\n",
       "0        1      4      8    2.9      1  Computer Science           History\n",
       "1        0      3      9    3.5      0              Math            French\n",
       "2        0      2      9    3.4      1              Math  Computer Science\n",
       "3        0      2      8    3.7      0      Data Science           English\n",
       "4        0      3      7    3.9      0      Data Science              Math\n",
       "5        1      3      7    3.2      0           English      Data Science\n",
       "6        1      4      8    2.9      1  Computer Science           History\n",
       "7        0      2      7    2.8      1      Data Science            French\n",
       "8        1      4      9    3.7      0  Computer Science            German\n",
       "9        0      3      6    3.1      1      Astrophysics              Math"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science', 'Math', 'Data Science', 'English', 'Astrophysics']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df['categ_1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 7 columns):\n",
      "dropout    10 non-null int64\n",
      "int_1      10 non-null int64\n",
      "int_2      10 non-null int64\n",
      "int_3      10 non-null float64\n",
      "int_4      10 non-null int64\n",
      "categ_1    10 non-null object\n",
      "categ_2    10 non-null object\n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 640.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categ_1'] = pd.Categorical(df['categ_1'], categories = list(df['categ_1'].unique())+['unk'])\n",
    "df['categ_2'] = pd.Categorical(df['categ_2'], categories = list(df['categ_2'].unique())+['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['categ_1', 'categ_2'], prefix = ['categ_1', 'categ_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout</th>\n",
       "      <th>int_1</th>\n",
       "      <th>int_2</th>\n",
       "      <th>int_3</th>\n",
       "      <th>int_4</th>\n",
       "      <th>categ_1_Computer Science</th>\n",
       "      <th>categ_1_Math</th>\n",
       "      <th>categ_1_Data Science</th>\n",
       "      <th>categ_1_English</th>\n",
       "      <th>categ_1_Astrophysics</th>\n",
       "      <th>categ_1_unk</th>\n",
       "      <th>categ_2_History</th>\n",
       "      <th>categ_2_French</th>\n",
       "      <th>categ_2_Computer Science</th>\n",
       "      <th>categ_2_English</th>\n",
       "      <th>categ_2_Math</th>\n",
       "      <th>categ_2_Data Science</th>\n",
       "      <th>categ_2_German</th>\n",
       "      <th>categ_2_unk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dropout  int_1  int_2  int_3  int_4  categ_1_Computer Science  \\\n",
       "0        1      4      8    2.9      1                         1   \n",
       "1        0      3      9    3.5      0                         0   \n",
       "2        0      2      9    3.4      1                         0   \n",
       "3        0      2      8    3.7      0                         0   \n",
       "4        0      3      7    3.9      0                         0   \n",
       "5        1      3      7    3.2      0                         0   \n",
       "6        1      4      8    2.9      1                         1   \n",
       "7        0      2      7    2.8      1                         0   \n",
       "8        1      4      9    3.7      0                         1   \n",
       "9        0      3      6    3.1      1                         0   \n",
       "\n",
       "   categ_1_Math  categ_1_Data Science  categ_1_English  categ_1_Astrophysics  \\\n",
       "0             0                     0                0                     0   \n",
       "1             1                     0                0                     0   \n",
       "2             1                     0                0                     0   \n",
       "3             0                     1                0                     0   \n",
       "4             0                     1                0                     0   \n",
       "5             0                     0                1                     0   \n",
       "6             0                     0                0                     0   \n",
       "7             0                     1                0                     0   \n",
       "8             0                     0                0                     0   \n",
       "9             0                     0                0                     1   \n",
       "\n",
       "   categ_1_unk  categ_2_History  categ_2_French  categ_2_Computer Science  \\\n",
       "0            0                1               0                         0   \n",
       "1            0                0               1                         0   \n",
       "2            0                0               0                         1   \n",
       "3            0                0               0                         0   \n",
       "4            0                0               0                         0   \n",
       "5            0                0               0                         0   \n",
       "6            0                1               0                         0   \n",
       "7            0                0               1                         0   \n",
       "8            0                0               0                         0   \n",
       "9            0                0               0                         0   \n",
       "\n",
       "   categ_2_English  categ_2_Math  categ_2_Data Science  categ_2_German  \\\n",
       "0                0             0                     0               0   \n",
       "1                0             0                     0               0   \n",
       "2                0             0                     0               0   \n",
       "3                1             0                     0               0   \n",
       "4                0             1                     0               0   \n",
       "5                0             0                     1               0   \n",
       "6                0             0                     0               0   \n",
       "7                0             0                     0               0   \n",
       "8                0             0                     0               1   \n",
       "9                0             1                     0               0   \n",
       "\n",
       "   categ_2_unk  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "toySPDF = sqlContext.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+------------------------+------------+--------------------+---------------+--------------------+-----------+---------------+--------------+------------------------+---------------+------------+--------------------+--------------+-----------+\n",
      "|dropout|int_1|int_2|int_3|int_4|categ_1_Computer Science|categ_1_Math|categ_1_Data Science|categ_1_English|categ_1_Astrophysics|categ_1_unk|categ_2_History|categ_2_French|categ_2_Computer Science|categ_2_English|categ_2_Math|categ_2_Data Science|categ_2_German|categ_2_unk|\n",
      "+-------+-----+-----+-----+-----+------------------------+------------+--------------------+---------------+--------------------+-----------+---------------+--------------+------------------------+---------------+------------+--------------------+--------------+-----------+\n",
      "|      1|    4|    8|  2.9|    1|                       1|           0|                   0|              0|                   0|          0|              1|             0|                       0|              0|           0|                   0|             0|          0|\n",
      "|      0|    3|    9|  3.5|    0|                       0|           1|                   0|              0|                   0|          0|              0|             1|                       0|              0|           0|                   0|             0|          0|\n",
      "|      0|    2|    9|  3.4|    1|                       0|           1|                   0|              0|                   0|          0|              0|             0|                       1|              0|           0|                   0|             0|          0|\n",
      "|      0|    2|    8|  3.7|    0|                       0|           0|                   1|              0|                   0|          0|              0|             0|                       0|              1|           0|                   0|             0|          0|\n",
      "|      0|    3|    7|  3.9|    0|                       0|           0|                   1|              0|                   0|          0|              0|             0|                       0|              0|           1|                   0|             0|          0|\n",
      "|      1|    3|    7|  3.2|    0|                       0|           0|                   0|              1|                   0|          0|              0|             0|                       0|              0|           0|                   1|             0|          0|\n",
      "|      1|    4|    8|  2.9|    1|                       1|           0|                   0|              0|                   0|          0|              1|             0|                       0|              0|           0|                   0|             0|          0|\n",
      "|      0|    2|    7|  2.8|    1|                       0|           0|                   1|              0|                   0|          0|              0|             1|                       0|              0|           0|                   0|             0|          0|\n",
      "|      1|    4|    9|  3.7|    0|                       1|           0|                   0|              0|                   0|          0|              0|             0|                       0|              0|           0|                   0|             1|          0|\n",
      "|      0|    3|    6|  3.1|    1|                       0|           0|                   0|              0|                   1|          0|              0|             0|                       0|              0|           1|                   0|             0|          0|\n",
      "+-------+-----+-----+-----+-----+------------------------+------------+--------------------+---------------+--------------------+-----------+---------------+--------------+------------------------+---------------+------------+--------------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toySPDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rawDF = sqlContext.read.text('toy.txt').withColumnRenamed(\"value\", \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|1,4,8,2.9,1,Compu...|\n",
      "|0,3,9,3.5,0,Math,...|\n",
      "|0,2,9,3.4,1,Math,...|\n",
      "|0,2,8,3.7,0,Data ...|\n",
      "|0,3,7,3.9,0,Data ...|\n",
      "|1,3,7,3.2,0,Engli...|\n",
      "|1,4,8,2.9,1,Compu...|\n",
      "|0,2,7,2.8,1,Data ...|\n",
      "|1,4,9,3.7,0,Compu...|\n",
      "|0,3,6,3.1,1,Astro...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toy_rawDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColList=[\"clicked_0_1\"]\n",
    "intColList=[\"int_1\", \"int_2\", \"int_3\", \"int_4\", \"int_5\", \"int_6\", \"int_7\", \"int_8\", \"int_9\", \"int_10\", \"int_11\", \"int_12\", \"int_13\"]\n",
    "categColList=[\"categ_1\", \"categ_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2 1 10\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "raw_train_df, raw_validation_df, raw_test_df = toy_rawDF.randomSplit(weights, seed)\n",
    "\n",
    "# Cache and count the DataFrames\n",
    "n_train = raw_train_df.cache().count()\n",
    "n_val = raw_validation_df.cache().count()\n",
    "n_test = raw_test_df.cache().count()\n",
    "print(n_train, n_val, n_test, str(n_train + n_val + n_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '4'), (1, '8'), (2, '2.9'), (3, '1'), (4, 'Computer Science'), (5, 'History')]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parse_point(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    values = point.split(',')[1:]\n",
    "    #values = filter(None, values)\n",
    "    indices = range(len(values))\n",
    "    return zip(indices,values)\n",
    "\n",
    "print(list(parse_point(toy_rawDF.select('text').first()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,4,8,2.9,1,Computer Science,History'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_rawDF.select('text').first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f5aaf792d48>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_point(toy_rawDF.select('text').first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, split\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType, FloatType, DoubleType\n",
    "\n",
    "\n",
    "parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),StructField('_2', StringType())])))\n",
    "\n",
    "def parse_raw_df(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and feature.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'feature' columns.   \n",
    "  \n",
    "    \"\"\"\n",
    "    return (raw_df.select(split(raw_df.text,',').getItem(0).cast(\"double\").alias('label'),\n",
    "                         parse_point_udf(raw_df.text).alias('features'))\n",
    "                        .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_point_udf1 = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()), StructField('_2', FloatType()), StructField('_3', FloatType()), StructField('_4', FloatType()), StructField('_5', FloatType()), StructField('_6', StringType())])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_df1(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and feature.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'feature' columns.   \n",
    "  \n",
    "    \"\"\"\n",
    "    return (raw_df.select(split(raw_df.text,',').getItem(0).cast(\"double\").alias('label'),\n",
    "                         parse_point_udf1(raw_df.text).alias('features'))\n",
    "                        .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, features=[Row(_1=0, _2='2'), Row(_1=1, _2='7'), Row(_1=2, _2='2.8'), Row(_1=3, _2='1'), Row(_1=4, _2='Data Science'), Row(_1=5, _2='French')])\n"
     ]
    }
   ],
   "source": [
    "parsed_train_df = parse_raw_df(raw_train_df)\n",
    "print(parsed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1190.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 1498, localhost, executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 6 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13$$anonfun$applyOrElse$1.apply(EvaluatePython.scala:160)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13.applyOrElse(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13.apply(EvaluatePython.scala:158)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:91)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:88)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$2$$anon$1.hasNext(InMemoryRelation.scala:149)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.GeneratedMethodAccessor185.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 6 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13$$anonfun$applyOrElse$1.apply(EvaluatePython.scala:160)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13.applyOrElse(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13.apply(EvaluatePython.scala:158)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:91)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:88)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$2$$anon$1.hasNext(InMemoryRelation.scala:149)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-c28405d93717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparsed_train_df1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_raw_df1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_train_df1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \"\"\"\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \"\"\"\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1190.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 1498, localhost, executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 6 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13$$anonfun$applyOrElse$1.apply(EvaluatePython.scala:160)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13.applyOrElse(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13.apply(EvaluatePython.scala:158)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:91)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:88)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$2$$anon$1.hasNext(InMemoryRelation.scala:149)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.GeneratedMethodAccessor185.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 6 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15$$anonfun$apply$15.applyOrElse(EvaluatePython.scala:184)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$15.apply(EvaluatePython.scala:180)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13$$anonfun$applyOrElse$1.apply(EvaluatePython.scala:160)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13$$anonfun$apply$13.applyOrElse(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.org$apache$spark$sql$execution$python$EvaluatePython$$nullSafeConvert(EvaluatePython.scala:208)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$makeFromJava$13.apply(EvaluatePython.scala:158)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:91)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$evaluate$2.apply(BatchEvalPythonExec.scala:88)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$2$$anon$1.hasNext(InMemoryRelation.scala:149)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1092)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "parsed_train_df1 = parse_raw_df1(raw_train_df)\n",
    "print(parsed_train_df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (explode, col)\n",
    "num_categories = (parsed_train_df\n",
    "                    .select(explode('features').alias('features'))\n",
    "                    .distinct()\n",
    "                    .select(col('features').getField('_1').alias('featureNumber'))\n",
    "                    .groupBy('featureNumber')\n",
    "                    .sum()\n",
    "                    .orderBy('featureNumber')\n",
    "                    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(featureNumber=0, sum(featureNumber)=0), Row(featureNumber=1, sum(featureNumber)=3), Row(featureNumber=2, sum(featureNumber)=12), Row(featureNumber=3, sum(featureNumber)=6), Row(featureNumber=4, sum(featureNumber)=16), Row(featureNumber=5, sum(featureNumber)=25)]\n"
     ]
    }
   ],
   "source": [
    "print(num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    indices = sorted([ohe_dict_broadcast.value[feat] for feat in raw_feats])\n",
    "    values = np.ones(len(raw_feats))\n",
    "    return SparseVector(num_ohe_feats,indices,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "\n",
    "def ohe_udf_generator(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "    length = len(ohe_dict_broadcast.value)\n",
    "    return udf(lambda x: one_hot_encoding(x, ohe_dict_broadcast, length), VectorUDT())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def create_one_hot_dict(input_df):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n",
    "            (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    input_distinct_feats_df = input_df.select(explode(input_df.features)).distinct()\n",
    "    input_ohe_dict = (input_distinct_feats_df\n",
    "                     .rdd\n",
    "                     .map(lambda r: tuple(r[0]))\n",
    "                     .zipWithIndex().collectAsMap())\n",
    "    return input_ohe_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(1, '9'): 0,\n",
       " (3, '1'): 1,\n",
       " (4, 'Math'): 2,\n",
       " (4, 'Data Science'): 3,\n",
       " (5, 'History'): 4,\n",
       " (0, '4'): 5,\n",
       " (2, '3.9'): 6,\n",
       " (5, 'Data Science'): 7,\n",
       " (5, 'French'): 8,\n",
       " (2, '3.5'): 9,\n",
       " (3, '0'): 10,\n",
       " (2, '2.9'): 11,\n",
       " (0, '3'): 12,\n",
       " (5, 'German'): 13,\n",
       " (5, 'Math'): 14,\n",
       " (1, '7'): 15,\n",
       " (1, '8'): 16,\n",
       " (4, 'English'): 17,\n",
       " (4, 'Computer Science'): 18,\n",
       " (2, '2.8'): 19,\n",
       " (2, '3.7'): 20,\n",
       " (2, '3.2'): 21,\n",
       " (0, '2'): 22}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr_ohe_dict = create_one_hot_dict(parsed_train_df)\n",
    "num_ctr_ohe_feats = len(ctr_ohe_dict)\n",
    "print(num_ctr_ohe_feats)\n",
    "ctr_ohe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(23,[1,3,8,15,19,...|\n",
      "|  0.0|(23,[3,6,10,12,14...|\n",
      "|  0.0|(23,[0,2,8,9,10,1...|\n",
      "|  1.0|(23,[7,10,12,15,1...|\n",
      "|  1.0|(23,[1,4,5,11,16,...|\n",
      "|  1.0|(23,[1,4,5,11,16,...|\n",
      "|  1.0|(23,[0,5,10,13,18...|\n",
      "+-----+--------------------+\n",
      "\n",
      "None\n",
      "[Row(label=0.0, features=SparseVector(23, {1: 1.0, 3: 1.0, 8: 1.0, 15: 1.0, 19: 1.0, 22: 1.0}))]\n"
     ]
    }
   ],
   "source": [
    "ohe_dict_broadcast = sc.broadcast(ctr_ohe_dict)\n",
    "ohe_dict_udf =  ohe_udf_generator(ohe_dict_broadcast)\n",
    "ohe_train_df =  parsed_train_df.select(parsed_train_df.label, ohe_dict_udf(parsed_train_df.features).alias('features'))\n",
    "#ohe_train_df.show(1)                  \n",
    "\n",
    "print(ohe_train_df.count())\n",
    "print(ohe_train_df.show())\n",
    "print(ohe_train_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ohe_train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_rdd = ohe_train_df \\\n",
    "                     .rdd \\\n",
    "                     .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(23, {1: 1.0, 3: 1.0, 8: 1.0, 15: 1.0, 19: 1.0, 22: 1.0})),\n",
       " Row(label=0.0, features=SparseVector(23, {3: 1.0, 6: 1.0, 10: 1.0, 12: 1.0, 14: 1.0, 15: 1.0}))]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_train_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.5714285714285714\n",
      "Variance: 0.2448979591836735\n"
     ]
    }
   ],
   "source": [
    "meanDropOut = ohe_train_rdd.map(lambda x: x[0]).mean()\n",
    "varDropOut = ohe_train_rdd.map(lambda x: x[0]).variance()\n",
    "print(f\"Mean: {meanDropOut}\")\n",
    "print(f\"Variance: {varDropOut}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = SparseVector(23, {1: 1.0, 3: 1.0, 8: 1.0, 15: 1.0, 19: 1.0, 22: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = np.append(meanDropOut, np.zeros(23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(v, weight[1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(1, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part b - OLS gradient descent function\n",
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        ############## YOUR CODE HERE #############\n",
    "        model = GDUpdate(trainRDD, model, learningRate)\n",
    "        training_loss = LogLoss(trainRDD, model) \n",
    "        test_loss = LogLoss(testRDD, model)\n",
    "        ############## (END) YOUR CODE #############\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error curves - RUN THIS CELL AS IS\n",
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Log loss')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <--- SOLUTION --->\n",
    "# part d - write function to compute loss (FILL IN MISSING CODE BELOW)\n",
    "def LogLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (x[0], np.append([1.0], x[1])))\n",
    "    ################## YOUR CODE HERE ##################\n",
    "    loss = augmentedData.map(lambda x: (-x[0] * np.log(sigmoid(W.dot(x[1]))) - (1 - x[0]) * np.log(1 - sigmoid(W.dot(x[1])))) ).mean()\n",
    "    ################## (END) YOUR CODE ##################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - function to perform a single GD step\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: ( x[0], np.append([1.0], x[1]))).cache()\n",
    "    \n",
    "    ################## YOUR CODE HERE ################# \n",
    "    grad = augmentedData.map(lambda x: (sigmoid(W.dot(x[1])) - x[0])*x[1]).mean()\n",
    "    new_model = W - learningRate * grad\n",
    "    ################## (END) YOUR CODE ################# \n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,\n",
       "  array([1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 1., 0., 0., 1.])),\n",
       " (0.0,\n",
       "  array([1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0.]))]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_train_rdd \\\n",
    "       .map(lambda x: (x[0], np.append([1.0], x[1]))) \\\n",
    "       .take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model loss: [0.5528949  0.69314718 0.69314718 0.69314718 0.69314718 0.69314718\n",
      " 0.69314718 0.69314718 0.69314718 0.69314718 0.69314718 0.69314718\n",
      " 0.69314718 0.69314718 0.69314718 0.69314718 0.69314718 0.69314718\n",
      " 0.69314718 0.69314718 0.69314718 0.69314718 0.69314718 0.69314718]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline model loss: {LogLoss(ohe_train_rdd, BASELINE)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE:  Loss = 0.6926036532203024\n",
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.6673236038103592\n",
      "Model: [0.565, -0.004, 0.001, -0.009, -0.018, 0.01, 0.015, -0.009, 0.005, -0.018, -0.009, -0.008, 0.01, -0.013, 0.005, -0.009, -0.013, 0.01, 0.005, 0.015, -0.009, 0.005, 0.005, -0.009]\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.6435514434651056\n",
      "Model: [0.558, -0.008, 0.002, -0.018, -0.036, 0.02, 0.03, -0.018, 0.01, -0.036, -0.018, -0.015, 0.02, -0.026, 0.01, -0.018, -0.026, 0.02, 0.01, 0.03, -0.018, 0.01, 0.01, -0.018]\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.6211927776964008\n",
      "Model: [0.552, -0.011, 0.003, -0.027, -0.053, 0.03, 0.045, -0.027, 0.016, -0.053, -0.027, -0.022, 0.03, -0.038, 0.015, -0.027, -0.038, 0.03, 0.016, 0.045, -0.027, 0.015, 0.016, -0.027]\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 0.6001561133777915\n",
      "Model: [0.547, -0.015, 0.004, -0.035, -0.07, 0.039, 0.059, -0.035, 0.021, -0.07, -0.035, -0.029, 0.039, -0.049, 0.02, -0.035, -0.049, 0.039, 0.021, 0.059, -0.035, 0.02, 0.021, -0.035]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.5803534787891058\n",
      "Model: [0.541, -0.018, 0.005, -0.043, -0.086, 0.048, 0.073, -0.043, 0.027, -0.087, -0.043, -0.035, 0.048, -0.06, 0.025, -0.043, -0.06, 0.048, 0.027, 0.073, -0.043, 0.025, 0.027, -0.043]\n"
     ]
    }
   ],
   "source": [
    "nSteps = 5\n",
    "model = BASELINE\n",
    "print(f\"BASELINE:  Loss = {LogLoss(ohe_train_rdd,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(ohe_train_rdd, model)\n",
    "    loss = LogLoss(ohe_train_rdd, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLtrain, LLtest, models = GradientDescent(ohe_train_rdd, testRDD, wInit, nSteps = 50)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
