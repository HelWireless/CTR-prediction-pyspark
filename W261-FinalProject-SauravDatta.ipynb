{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference :\n",
    "#RandomForest\n",
    "#https://github.com/apache/spark/blob/v2.2.0/mllib/src/main/scala/org/apache/spark/ml/classification/RandomForestClassifier.scala#L120\n",
    "#One hot encoding\n",
    "#https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "#LR from scratch\n",
    "#https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 14.8MB/s ta 0:00:01\n",
      "\u001b[31mpyspark 2.3.1 requires py4j==0.10.7, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-18.1\n",
      "Requirement already up-to-date: pandas in /opt/conda/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "Collecting google-api-python-client\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/80/e034392c8190876167b4323aa497a3745e183b49b22cd2079c7d2f36c776/google_api_python_client-1.7.6-py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.0MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.0.3 (from google-api-python-client)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (1.11.0)\n",
      "Collecting google-auth>=1.4.1 (from google-api-python-client)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/62/8b9612b1055cfbecd577e252446fe5f939f6818d0b7ddc27bb872f233cd4/google_auth-1.6.1-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 9.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0 (from google-api-python-client)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/7d/9d5a640c4f8bf2c8b1afc015e9a9d8de32e13c9016dcc4b0ec03481fb396/uritemplate-3.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (0.12.0)\n",
      "Collecting cachetools>=2.0.0 (from google-auth>=1.4.1->google-api-python-client)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/7e/08cd3846bebeabb6b1cfc4af8aae649d90249b4aeed080bddb5297f1d73b/cachetools-3.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4)\n",
      "Installing collected packages: cachetools, google-auth, google-auth-httplib2, uritemplate, google-api-python-client\n",
      "Successfully installed cachetools-3.0.0 google-api-python-client-1.7.6 google-auth-1.6.1 google-auth-httplib2-0.0.3 uritemplate-3.0.0\n",
      "Collecting seaborn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 33.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.15.2 in /opt/conda/lib/python3.6/site-packages (from seaborn) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.15.2->seaborn) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.15.2->seaborn) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.2.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.9.0\n",
      "Collecting networkx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.7MB 17.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx) (4.3.0)\n",
      "Building wheels for collected packages: networkx\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
      "Successfully built networkx\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-2.2\n",
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 2.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.2.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Found existing installation: matplotlib 3.0.1\n",
      "    Uninstalling matplotlib-3.0.1:\n",
      "      Successfully uninstalled matplotlib-3.0.1\n",
      "Successfully installed matplotlib-3.0.2\n",
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 213.4MB 164kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 43.1MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Found existing installation: pyspark 2.3.1\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.0\n",
      "Collecting pyspark_dist_explore\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/c2/2f19468300f7c9d25dc526f0c11779d87e1a7c07d999347cf71a13282c0b/pyspark_dist_explore-0.1.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas->pyspark_dist_explore) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas->pyspark_dist_explore) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->pyspark_dist_explore) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->pyspark_dist_explore) (39.2.0)\n",
      "Installing collected packages: pyspark-dist-explore\n",
      "Successfully installed pyspark-dist-explore-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --upgrade networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade pyspark\n",
    "!pip install --upgrade pyspark_dist_explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display, HTML, display_html #usefull to display wide tables\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram\n",
    "from pyspark.sql import functions as F, types\n",
    "import datetime\n",
    "from pyspark.sql.functions import col, row_number, concat, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import trim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-w261-m.c.w266-203603.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff441856f60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.eventLog.dir', 'hdfs://cluster-w261-m/user/spark/eventlog'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1544374336644_0001'),\n",
       " ('spark.driver.port', '42569'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://cluster-w261-m.c.w266-203603.internal:4040'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'cluster-w261-m'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.historyServer.address', 'cluster-w261-m:18080'),\n",
       " ('spark.driver.maxResultSize', '3840m'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'hdfs://cluster-w261-m/user/spark/eventlog'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.memory', '11171m'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance'),\n",
       " ('spark.driver.host', 'cluster-w261-m.c.w266-203603.internal'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://cluster-w261-m:8088/proxy/application_1544374336644_0001'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.executorEnv.PYTHONHASHSEED', '0'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'application_1544374336644_0001'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.sql.parquet.cacheMetadata', 'false'),\n",
       " ('spark.driver.memory', '7680m'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.cbo.enabled', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\n",
    "\n",
    "#The number of cores can be specified with the --executor-cores flag when invoking \n",
    "#spark-submit, spark-shell, and pyspark from the command line, \n",
    "#or by setting the spark.executor.cores property in the spark-defaults.conf file \n",
    "#or on a SparkConf object.\n",
    "\n",
    "#The cores property controls the number of concurrent tasks an executor can run. \n",
    "#--executor-cores 5 means that each executor can run a maximum of five tasks at the same time.\n",
    "\n",
    "#The heap size can be controlled with the --executor-memory flag \n",
    "#or the spark.executor.memory property\n",
    "#The memory property impacts the amount of data Spark can cache, \n",
    "#as well as the maximum sizes of the shuffle data structures used for grouping, aggregations, and joins.\n",
    "\n",
    "#The --num-executors command-line flag or spark.executor.instances configuration property \n",
    "#control the number of executors requested. Starting in CDH 5.4/Spark 1.3, \n",
    "#you will be able to avoid setting this property by turning on dynamic allocation with the spark.dynamicAllocation.enabled property. Dynamic allocation enables a Spark application to request executors \n",
    "#when there is a backlog of pending tasks and free up executors when idle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.memory\", '19g')\n",
    "# spark.conf.set('spark.executor.cores', '5')\n",
    "# sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_calc_stats(data, column):\n",
    "        return data.agg(F.avg(data[column]), F.min(data[column]), F.max(data[column]), \\\n",
    "                        F.stddev_pop(data[column]),F.var_pop(data[column]),F.skewness(data[column]) \\\n",
    "                       ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_check_null(data, column):\n",
    "    return data.filter( (data[column] ==\"\") |F.isnull(data[column])|F.isnan(data[column])\n",
    "                      ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking cardinality\n",
    "def f_display_stats_categ(df, inColList):\n",
    "    dict1={}\n",
    "    for col in inColList:\n",
    "        cardinal_cnt = df.select([col]).distinct().count()\n",
    "        dict1[col]={\"Count_Unique_Vals\":cardinal_cnt}\n",
    "        dict1[col]['Count_Empty_String'] = str(df.filter(df[col] == \"\").count())\n",
    "\n",
    "    display(HTML(pd.DataFrame(dict1).T.to_html( )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display_stats_int(data):\n",
    "    dict1={}\n",
    "    countTotal = data.count()\n",
    "    for colname in [item[0] for item in data.dtypes if item[1].startswith('int')]:\n",
    "        list1=f_calc_stats(data,colname)\n",
    "        mean_val, min_val,max_val,stddev,var, skewness =list1[0]\n",
    "        count_nulls = f_check_null(data,colname)\n",
    "        dict1[colname]={}\n",
    "        dict1[colname]['mean'] = str(round(mean_val,2))\n",
    "        dict1[colname]['min'] = str(min_val)\n",
    "        dict1[colname]['max'] = str(max_val)\n",
    "        dict1[colname]['stddev'] = str(round(stddev,2))\n",
    "        dict1[colname]['var'] = str(round(var,2))\n",
    "        dict1[colname]['skewness'] = str(round(skewness,2))\n",
    "        dict1[colname]['nulls_nans'] = str(count_nulls)\n",
    "        dict1[colname]['pct_nulls_nans'] = str(round(float(count_nulls/countTotal*100),2))\n",
    "        dict1[colname]['count_empty_string'] = str(data.filter(data[colname] == \"\").count())\n",
    "        dict1[colname]['count_unique_values'] = str(data.select([colname]).distinct().count())\n",
    "        dict1[colname]['count'] = 'N/A'\n",
    "    dict1['TOTAL']={}\n",
    "    dict1['TOTAL']['count'] = str(data.count())\n",
    "   #Transposing dataframe to keep column names as rows\n",
    "    display(HTML(pd.DataFrame(dict1).T.to_html( )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from RDD\n",
    "def f_covert_to_df(rdd, col_list):\n",
    "    return rdd.map(lambda x: x.split('\\t')).toDF(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert integer columns to IntegerType from String\n",
    "def f_cast_str_to_int(df, integer_col_list):\n",
    "    for col in integer_col_list:\n",
    "        df = df.withColumn(col, df[col].cast(types.IntegerType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows where at least one column has empty string\n",
    "def f_remove_empty_string(df, categ_col_list):\n",
    "    for categ_col in categ_col_list:\n",
    "        df = df.filter(df[categ_col] != \"\")\n",
    "        #df = df.filter(col(categ_col) != \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting categorical variables to one-hot encoding\n",
    "def f_onehot_encoding(df,label_col_list, int_col_list, categ_col_list):\n",
    "    stages=[]\n",
    "    for categoricalCol in categ_col_list:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "#         encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        encoder = OneHotEncoderEstimator(handleInvalid='keep', inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "        \n",
    "    #Skip for test data since test data does not have label\n",
    "    if label_col_list is not None:\n",
    "        labelColStr=''.join(label_col_list)\n",
    "        label_stringIdx = StringIndexer(inputCol = labelColStr, outputCol = 'label')\n",
    "        stages += [label_stringIdx]\n",
    "\n",
    "    assemblerInputs = [c + \"classVec\" for c in categ_col_list] + int_col_list\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "\n",
    "    cols = df.columns\n",
    "\n",
    "#     partialPipeline = Pipeline(stages = stages)\n",
    "    partialPipeline = Pipeline().setStages(stages)\n",
    "    pipelineModel = partialPipeline.fit(df)\n",
    "\n",
    "    preppedDataDF = pipelineModel.transform(df)\n",
    "\n",
    "#     if label_col_list is not None:\n",
    "#          selectedCols = ['label', 'features'] + cols\n",
    "#     else:\n",
    "#         selectedCols = ['features'] + cols\n",
    "        \n",
    "#     preppedDataDF = preppedDataDF.select(selectedCols)\n",
    "\n",
    "    return preppedDataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gs://bucket-w261-final/data/train.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD = sc.textFile('gs://bucket-w261-final/data/train.txt',50)\n",
    "trainRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gs://bucket-w261-final/data/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD = sc.textFile('gs://bucket-w261-final/data/test.txt',50)\n",
    "testRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColList=[\"clicked_0_1\"]\n",
    "intColList=[\"int_1\", \"int_2\", \"int_3\", \"int_4\", \"int_5\", \"int_6\", \"int_7\", \"int_8\", \"int_9\", \"int_10\", \"int_11\", \"int_12\", \"int_13\"]\n",
    "categColList=[\"categ_1\", \"categ_2\", \"categ_3\", \"categ_4\", \"categ_5\", \"categ_6\", \"categ_7\", \"categ_8\", \"categ_9\", \"categ_10\", \"categ_11\", \"categ_12\", \"categ_13\", \"categ_14\", \"categ_15\", \"categ_16\", \"categ_17\", \"categ_18\", \"categ_19\", \"categ_20\", \"categ_21\", \"categ_22\", \"categ_23\", \"categ_24\", \"categ_25\", \"categ_26\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[clicked_0_1: string, int_1: string, int_2: string, int_3: string, int_4: string, int_5: string, int_6: string, int_7: string, int_8: string, int_9: string, int_10: string, int_11: string, int_12: string, int_13: string, categ_1: string, categ_2: string, categ_3: string, categ_4: string, categ_5: string, categ_6: string, categ_7: string, categ_8: string, categ_9: string, categ_10: string, categ_11: string, categ_12: string, categ_13: string, categ_14: string, categ_15: string, categ_16: string, categ_17: string, categ_18: string, categ_19: string, categ_20: string, categ_21: string, categ_22: string, categ_23: string, categ_24: string, categ_25: string, categ_26: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colList=[]\n",
    "colList=labelColList+intColList+categColList\n",
    "trainDF = f_covert_to_df(trainRDD,colList)\n",
    "trainDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[int_1: string, int_2: string, int_3: string, int_4: string, int_5: string, int_6: string, int_7: string, int_8: string, int_9: string, int_10: string, int_11: string, int_12: string, int_13: string, categ_1: string, categ_2: string, categ_3: string, categ_4: string, categ_5: string, categ_6: string, categ_7: string, categ_8: string, categ_9: string, categ_10: string, categ_11: string, categ_12: string, categ_13: string, categ_14: string, categ_15: string, categ_16: string, categ_17: string, categ_18: string, categ_19: string, categ_20: string, categ_21: string, categ_22: string, categ_23: string, categ_24: string, categ_25: string, categ_26: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colList=[]\n",
    "colList=intColList+categColList\n",
    "testDF = f_covert_to_df(testRDD,colList)\n",
    "testDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[clicked_0_1: string, int_1: int, int_2: int, int_3: int, int_4: int, int_5: int, int_6: int, int_7: int, int_8: int, int_9: int, int_10: int, int_11: int, int_12: int, int_13: int, categ_1: string, categ_2: string, categ_3: string, categ_4: string, categ_5: string, categ_6: string, categ_7: string, categ_8: string, categ_9: string, categ_10: string, categ_11: string, categ_12: string, categ_13: string, categ_14: string, categ_15: string, categ_16: string, categ_17: string, categ_18: string, categ_19: string, categ_20: string, categ_21: string, categ_22: string, categ_23: string, categ_24: string, categ_25: string, categ_26: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = f_cast_str_to_int(trainDF,intColList)\n",
    "trainDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[int_1: int, int_2: int, int_3: int, int_4: int, int_5: int, int_6: int, int_7: int, int_8: int, int_9: int, int_10: int, int_11: int, int_12: int, int_13: int, categ_1: string, categ_2: string, categ_3: string, categ_4: string, categ_5: string, categ_6: string, categ_7: string, categ_8: string, categ_9: string, categ_10: string, categ_11: string, categ_12: string, categ_13: string, categ_14: string, categ_15: string, categ_16: string, categ_17: string, categ_18: string, categ_19: string, categ_20: string, categ_21: string, categ_22: string, categ_23: string, categ_24: string, categ_25: string, categ_26: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = f_cast_str_to_int(testDF,intColList)\n",
    "testDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9308"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a subset for toy example\n",
    "#intSubColList=[\"int_1\",\"int_2\"]\n",
    "#categSubColList=[\"categ_1\",\"categ_2\"]\n",
    "trainsubsetDF=trainDF.sample( withReplacement=True,fraction=0.0002,seed=1)\n",
    "trainsubsetDF.cache()\n",
    "trainsubsetDF.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testsubsetDF=testDF.sample( withReplacement=True,fraction=0.0002,seed=1)\n",
    "testsubsetDF.cache()\n",
    "testsubsetDF.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows where at least one column has empty string\n",
    "trainsubsetDF = trainsubsetDF.dropna()\n",
    "testsubsetDF  = testsubsetDF.dropna()\n",
    "trainsubsetDF= f_remove_empty_string(trainsubsetDF, categColList)\n",
    "testsubsetDF= f_remove_empty_string(testsubsetDF, categColList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[int_1: int, int_2: int, int_3: int, int_4: int, int_5: int, int_6: int, int_7: int, int_8: int, int_9: int, int_10: int, int_11: int, int_12: int, int_13: int, categ_1: string, categ_2: string, categ_3: string, categ_4: string, categ_5: string, categ_6: string, categ_7: string, categ_8: string, categ_9: string, categ_10: string, categ_11: string, categ_12: string, categ_13: string, categ_14: string, categ_15: string, categ_16: string, categ_17: string, categ_18: string, categ_19: string, categ_20: string, categ_21: string, categ_22: string, categ_23: string, categ_24: string, categ_25: string, categ_26: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainsubsetDF.cache()\n",
    "testsubsetDF.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count_empty_string</th>\n",
       "      <th>count_unique_values</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>nulls_nans</th>\n",
       "      <th>pct_nulls_nans</th>\n",
       "      <th>skewness</th>\n",
       "      <th>stddev</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>int_1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>81</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>11.04</td>\n",
       "      <td>121.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2468</td>\n",
       "      <td>90.07</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.39</td>\n",
       "      <td>303.71</td>\n",
       "      <td>92239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>244</td>\n",
       "      <td>22.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>31.95</td>\n",
       "      <td>1020.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>56</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>8.7</td>\n",
       "      <td>75.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>33255</td>\n",
       "      <td>1706.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.24</td>\n",
       "      <td>3408.72</td>\n",
       "      <td>11619382.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>889</td>\n",
       "      <td>68.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.98</td>\n",
       "      <td>131.79</td>\n",
       "      <td>17369.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>470</td>\n",
       "      <td>43.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.38</td>\n",
       "      <td>78.41</td>\n",
       "      <td>6147.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>18.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>14.38</td>\n",
       "      <td>206.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>2792</td>\n",
       "      <td>260.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.83</td>\n",
       "      <td>446.73</td>\n",
       "      <td>199564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_10</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_11</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>6.74</td>\n",
       "      <td>45.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_12</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.77</td>\n",
       "      <td>4.85</td>\n",
       "      <td>23.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_13</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>56</td>\n",
       "      <td>9.17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>10.04</td>\n",
       "      <td>100.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count_Empty_String</th>\n",
       "      <th>Count_Unique_Vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>categ_1</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_2</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_3</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_4</th>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_5</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_6</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_7</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_8</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_10</th>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_11</th>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_12</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_13</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_14</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_15</th>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_16</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_17</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_18</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_19</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_20</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_21</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_22</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_23</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_24</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_25</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_26</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 764 ms, total: 1.95 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f_display_stats_int(trainsubsetDF)\n",
    "f_display_stats_categ(trainsubsetDF, categColList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- clicked_0_1: string (nullable = true)\n",
      " |-- int_1: integer (nullable = true)\n",
      " |-- int_2: integer (nullable = true)\n",
      " |-- int_3: integer (nullable = true)\n",
      " |-- int_4: integer (nullable = true)\n",
      " |-- int_5: integer (nullable = true)\n",
      " |-- int_6: integer (nullable = true)\n",
      " |-- int_7: integer (nullable = true)\n",
      " |-- int_8: integer (nullable = true)\n",
      " |-- int_9: integer (nullable = true)\n",
      " |-- int_10: integer (nullable = true)\n",
      " |-- int_11: integer (nullable = true)\n",
      " |-- int_12: integer (nullable = true)\n",
      " |-- int_13: integer (nullable = true)\n",
      " |-- categ_1: string (nullable = true)\n",
      " |-- categ_2: string (nullable = true)\n",
      " |-- categ_3: string (nullable = true)\n",
      " |-- categ_4: string (nullable = true)\n",
      " |-- categ_5: string (nullable = true)\n",
      " |-- categ_6: string (nullable = true)\n",
      " |-- categ_7: string (nullable = true)\n",
      " |-- categ_8: string (nullable = true)\n",
      " |-- categ_9: string (nullable = true)\n",
      " |-- categ_10: string (nullable = true)\n",
      " |-- categ_11: string (nullable = true)\n",
      " |-- categ_12: string (nullable = true)\n",
      " |-- categ_13: string (nullable = true)\n",
      " |-- categ_14: string (nullable = true)\n",
      " |-- categ_15: string (nullable = true)\n",
      " |-- categ_16: string (nullable = true)\n",
      " |-- categ_17: string (nullable = true)\n",
      " |-- categ_18: string (nullable = true)\n",
      " |-- categ_19: string (nullable = true)\n",
      " |-- categ_20: string (nullable = true)\n",
      " |-- categ_21: string (nullable = true)\n",
      " |-- categ_22: string (nullable = true)\n",
      " |-- categ_23: string (nullable = true)\n",
      " |-- categ_24: string (nullable = true)\n",
      " |-- categ_25: string (nullable = true)\n",
      " |-- categ_26: string (nullable = true)\n",
      " |-- categ_1Index: double (nullable = false)\n",
      " |-- categ_1classVec: vector (nullable = true)\n",
      " |-- categ_2Index: double (nullable = false)\n",
      " |-- categ_2classVec: vector (nullable = true)\n",
      " |-- categ_3Index: double (nullable = false)\n",
      " |-- categ_3classVec: vector (nullable = true)\n",
      " |-- categ_4Index: double (nullable = false)\n",
      " |-- categ_4classVec: vector (nullable = true)\n",
      " |-- categ_5Index: double (nullable = false)\n",
      " |-- categ_5classVec: vector (nullable = true)\n",
      " |-- categ_6Index: double (nullable = false)\n",
      " |-- categ_6classVec: vector (nullable = true)\n",
      " |-- categ_7Index: double (nullable = false)\n",
      " |-- categ_7classVec: vector (nullable = true)\n",
      " |-- categ_8Index: double (nullable = false)\n",
      " |-- categ_8classVec: vector (nullable = true)\n",
      " |-- categ_9Index: double (nullable = false)\n",
      " |-- categ_9classVec: vector (nullable = true)\n",
      " |-- categ_10Index: double (nullable = false)\n",
      " |-- categ_10classVec: vector (nullable = true)\n",
      " |-- categ_11Index: double (nullable = false)\n",
      " |-- categ_11classVec: vector (nullable = true)\n",
      " |-- categ_12Index: double (nullable = false)\n",
      " |-- categ_12classVec: vector (nullable = true)\n",
      " |-- categ_13Index: double (nullable = false)\n",
      " |-- categ_13classVec: vector (nullable = true)\n",
      " |-- categ_14Index: double (nullable = false)\n",
      " |-- categ_14classVec: vector (nullable = true)\n",
      " |-- categ_15Index: double (nullable = false)\n",
      " |-- categ_15classVec: vector (nullable = true)\n",
      " |-- categ_16Index: double (nullable = false)\n",
      " |-- categ_16classVec: vector (nullable = true)\n",
      " |-- categ_17Index: double (nullable = false)\n",
      " |-- categ_17classVec: vector (nullable = true)\n",
      " |-- categ_18Index: double (nullable = false)\n",
      " |-- categ_18classVec: vector (nullable = true)\n",
      " |-- categ_19Index: double (nullable = false)\n",
      " |-- categ_19classVec: vector (nullable = true)\n",
      " |-- categ_20Index: double (nullable = false)\n",
      " |-- categ_20classVec: vector (nullable = true)\n",
      " |-- categ_21Index: double (nullable = false)\n",
      " |-- categ_21classVec: vector (nullable = true)\n",
      " |-- categ_22Index: double (nullable = false)\n",
      " |-- categ_22classVec: vector (nullable = true)\n",
      " |-- categ_23Index: double (nullable = false)\n",
      " |-- categ_23classVec: vector (nullable = true)\n",
      " |-- categ_24Index: double (nullable = false)\n",
      " |-- categ_24classVec: vector (nullable = true)\n",
      " |-- categ_25Index: double (nullable = false)\n",
      " |-- categ_25classVec: vector (nullable = true)\n",
      " |-- categ_26Index: double (nullable = false)\n",
      " |-- categ_26classVec: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainOHEdf=f_onehot_encoding(trainsubsetDF, labelColList, intColList,categColList )\n",
    "trainOHEdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- int_1: integer (nullable = true)\n",
      " |-- int_2: integer (nullable = true)\n",
      " |-- int_3: integer (nullable = true)\n",
      " |-- int_4: integer (nullable = true)\n",
      " |-- int_5: integer (nullable = true)\n",
      " |-- int_6: integer (nullable = true)\n",
      " |-- int_7: integer (nullable = true)\n",
      " |-- int_8: integer (nullable = true)\n",
      " |-- int_9: integer (nullable = true)\n",
      " |-- int_10: integer (nullable = true)\n",
      " |-- int_11: integer (nullable = true)\n",
      " |-- int_12: integer (nullable = true)\n",
      " |-- int_13: integer (nullable = true)\n",
      " |-- categ_1: string (nullable = true)\n",
      " |-- categ_2: string (nullable = true)\n",
      " |-- categ_3: string (nullable = true)\n",
      " |-- categ_4: string (nullable = true)\n",
      " |-- categ_5: string (nullable = true)\n",
      " |-- categ_6: string (nullable = true)\n",
      " |-- categ_7: string (nullable = true)\n",
      " |-- categ_8: string (nullable = true)\n",
      " |-- categ_9: string (nullable = true)\n",
      " |-- categ_10: string (nullable = true)\n",
      " |-- categ_11: string (nullable = true)\n",
      " |-- categ_12: string (nullable = true)\n",
      " |-- categ_13: string (nullable = true)\n",
      " |-- categ_14: string (nullable = true)\n",
      " |-- categ_15: string (nullable = true)\n",
      " |-- categ_16: string (nullable = true)\n",
      " |-- categ_17: string (nullable = true)\n",
      " |-- categ_18: string (nullable = true)\n",
      " |-- categ_19: string (nullable = true)\n",
      " |-- categ_20: string (nullable = true)\n",
      " |-- categ_21: string (nullable = true)\n",
      " |-- categ_22: string (nullable = true)\n",
      " |-- categ_23: string (nullable = true)\n",
      " |-- categ_24: string (nullable = true)\n",
      " |-- categ_25: string (nullable = true)\n",
      " |-- categ_26: string (nullable = true)\n",
      " |-- categ_1Index: double (nullable = false)\n",
      " |-- categ_1classVec: vector (nullable = true)\n",
      " |-- categ_2Index: double (nullable = false)\n",
      " |-- categ_2classVec: vector (nullable = true)\n",
      " |-- categ_3Index: double (nullable = false)\n",
      " |-- categ_3classVec: vector (nullable = true)\n",
      " |-- categ_4Index: double (nullable = false)\n",
      " |-- categ_4classVec: vector (nullable = true)\n",
      " |-- categ_5Index: double (nullable = false)\n",
      " |-- categ_5classVec: vector (nullable = true)\n",
      " |-- categ_6Index: double (nullable = false)\n",
      " |-- categ_6classVec: vector (nullable = true)\n",
      " |-- categ_7Index: double (nullable = false)\n",
      " |-- categ_7classVec: vector (nullable = true)\n",
      " |-- categ_8Index: double (nullable = false)\n",
      " |-- categ_8classVec: vector (nullable = true)\n",
      " |-- categ_9Index: double (nullable = false)\n",
      " |-- categ_9classVec: vector (nullable = true)\n",
      " |-- categ_10Index: double (nullable = false)\n",
      " |-- categ_10classVec: vector (nullable = true)\n",
      " |-- categ_11Index: double (nullable = false)\n",
      " |-- categ_11classVec: vector (nullable = true)\n",
      " |-- categ_12Index: double (nullable = false)\n",
      " |-- categ_12classVec: vector (nullable = true)\n",
      " |-- categ_13Index: double (nullable = false)\n",
      " |-- categ_13classVec: vector (nullable = true)\n",
      " |-- categ_14Index: double (nullable = false)\n",
      " |-- categ_14classVec: vector (nullable = true)\n",
      " |-- categ_15Index: double (nullable = false)\n",
      " |-- categ_15classVec: vector (nullable = true)\n",
      " |-- categ_16Index: double (nullable = false)\n",
      " |-- categ_16classVec: vector (nullable = true)\n",
      " |-- categ_17Index: double (nullable = false)\n",
      " |-- categ_17classVec: vector (nullable = true)\n",
      " |-- categ_18Index: double (nullable = false)\n",
      " |-- categ_18classVec: vector (nullable = true)\n",
      " |-- categ_19Index: double (nullable = false)\n",
      " |-- categ_19classVec: vector (nullable = true)\n",
      " |-- categ_20Index: double (nullable = false)\n",
      " |-- categ_20classVec: vector (nullable = true)\n",
      " |-- categ_21Index: double (nullable = false)\n",
      " |-- categ_21classVec: vector (nullable = true)\n",
      " |-- categ_22Index: double (nullable = false)\n",
      " |-- categ_22classVec: vector (nullable = true)\n",
      " |-- categ_23Index: double (nullable = false)\n",
      " |-- categ_23classVec: vector (nullable = true)\n",
      " |-- categ_24Index: double (nullable = false)\n",
      " |-- categ_24classVec: vector (nullable = true)\n",
      " |-- categ_25Index: double (nullable = false)\n",
      " |-- categ_25classVec: vector (nullable = true)\n",
      " |-- categ_26Index: double (nullable = false)\n",
      " |-- categ_26classVec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testOHEdf=f_onehot_encoding(testsubsetDF, None, intColList,categColList )\n",
    "testOHEdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexerList = [StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index') for categoricalCol in categColList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderList = [OneHotEncoderEstimator(handleInvalid='keep', inputCols=[stringIndexer.getOutputCol()], outputCols=[stringIndexer.getInputCol() + \"classVec\"]) for stringIndexer in stringIndexerList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages=[]\n",
    "stages += [stringIndexerList]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColStr=''.join(labelColList)\n",
    "label_stringIdx = StringIndexer(inputCol = labelColStr, outputCol = 'label')\n",
    "stages += [label_stringIdx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = trainsubsetDF.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "setStages() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-32644314da70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpartialPipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetStages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartialPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsubsetDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: setStages() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "partialPipeline = Pipeline.setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(trainsubsetDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[int_1: int, int_2: int, int_3: int, int_4: int, int_5: int, int_6: int, int_7: int, int_8: int, int_9: int, int_10: int, int_11: int, int_12: int, int_13: int, categ_1: string, categ_2: string, categ_3: string, categ_4: string, categ_5: string, categ_6: string, categ_7: string, categ_8: string, categ_9: string, categ_10: string, categ_11: string, categ_12: string, categ_13: string, categ_14: string, categ_15: string, categ_16: string, categ_17: string, categ_18: string, categ_19: string, categ_20: string, categ_21: string, categ_22: string, categ_23: string, categ_24: string, categ_25: string, categ_26: string, categ_1Index: double, categ_1classVec: vector, categ_2Index: double, categ_2classVec: vector, categ_3Index: double, categ_3classVec: vector, categ_4Index: double, categ_4classVec: vector, categ_5Index: double, categ_5classVec: vector, categ_6Index: double, categ_6classVec: vector, categ_7Index: double, categ_7classVec: vector, categ_8Index: double, categ_8classVec: vector, categ_9Index: double, categ_9classVec: vector, categ_10Index: double, categ_10classVec: vector, categ_11Index: double, categ_11classVec: vector, categ_12Index: double, categ_12classVec: vector, categ_13Index: double, categ_13classVec: vector, categ_14Index: double, categ_14classVec: vector, categ_15Index: double, categ_15classVec: vector, categ_16Index: double, categ_16classVec: vector, categ_17Index: double, categ_17classVec: vector, categ_18Index: double, categ_18classVec: vector, categ_19Index: double, categ_19classVec: vector, categ_20Index: double, categ_20classVec: vector, categ_21Index: double, categ_21classVec: vector, categ_22Index: double, categ_22classVec: vector, categ_23Index: double, categ_23classVec: vector, categ_24Index: double, categ_24classVec: vector, categ_25Index: double, categ_25classVec: vector, categ_26Index: double, categ_26classVec: vector, features: vector]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainOHEdf.cache()\n",
    "testOHEdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(trainOHEdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "??LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testOHEdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'int_1, int_2, int_3, int_4, int_5, int_6, int_7, int_8, int_9, int_10, int_11, int_12, int_13, categ_1, categ_2, categ_3, categ_4, categ_5, categ_6, categ_7, categ_8, categ_9, categ_10, categ_11, categ_12, categ_13, categ_14, categ_15, categ_16, categ_17, categ_18, categ_19, categ_20, categ_21, categ_22, categ_23, categ_24, categ_25, categ_26, categ_1Index, categ_1classVec, categ_2Index, categ_2classVec, categ_3Index, categ_3classVec, categ_4Index, categ_4classVec, categ_5Index, categ_5classVec, categ_6Index, categ_6classVec, categ_7Index, categ_7classVec, categ_8Index, categ_8classVec, categ_9Index, categ_9classVec, categ_10Index, categ_10classVec, categ_11Index, categ_11classVec, categ_12Index, categ_12classVec, categ_13Index, categ_13classVec, categ_14Index, categ_14classVec, categ_15Index, categ_15classVec, categ_16Index, categ_16classVec, categ_17Index, categ_17classVec, categ_18Index, categ_18classVec, categ_19Index, categ_19classVec, categ_20Index, categ_20classVec, categ_21Index, categ_21classVec, categ_22Index, categ_22classVec, categ_23Index, categ_23classVec, categ_24Index, categ_24classVec, categ_25Index, categ_25classVec, categ_26Index, categ_26classVec, features, rawPrediction, probability, prediction'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(list(predictions.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7831.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 784.0 failed 4 times, most recent failure: Lost task 3.3 in stage 784.0 (TID 46486, cluster-w261-w-5.c.w266-203603.internal, executor 16): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 452, y.size = 1656\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 452, y.size = 1656\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-eaf8c1667b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o7831.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 784.0 failed 4 times, most recent failure: Lost task 3.3 in stage 784.0 (TID 46486, cluster-w261-w-5.c.w266-203603.internal, executor 16): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 452, y.size = 1656\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 452, y.size = 1656\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "predictions.select(['prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+---------------+------------+---------------+------------+----------------+------------+-----------------+------------+---------------+------------+---------------+------------+----------------+------------+---------------+------------+---------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+-----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+-----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|clicked_0_1|int_1|int_2|int_3|int_4|int_5|int_6|int_7|int_8|int_9|int_10|int_11|int_12|int_13|categ_1 |categ_2 |categ_3 |categ_4 |categ_5 |categ_6 |categ_7 |categ_8 |categ_9 |categ_10|categ_11|categ_12|categ_13|categ_14|categ_15|categ_16|categ_17|categ_18|categ_19|categ_20|categ_21|categ_22|categ_23|categ_24|categ_25|categ_26|categ_1Index|categ_1classVec|categ_2Index|categ_2classVec|categ_3Index|categ_3classVec |categ_4Index|categ_4classVec  |categ_5Index|categ_5classVec|categ_6Index|categ_6classVec|categ_7Index|categ_7classVec |categ_8Index|categ_8classVec|categ_9Index|categ_9classVec|categ_10Index|categ_10classVec|categ_11Index|categ_11classVec|categ_12Index|categ_12classVec|categ_13Index|categ_13classVec|categ_14Index|categ_14classVec|categ_15Index|categ_15classVec|categ_16Index|categ_16classVec |categ_17Index|categ_17classVec|categ_18Index|categ_18classVec|categ_19Index|categ_19classVec|categ_20Index|categ_20classVec|categ_21Index|categ_21classVec |categ_22Index|categ_22classVec|categ_23Index|categ_23classVec|categ_24Index|categ_24classVec|categ_25Index|categ_25classVec|categ_26Index|categ_26classVec|label|features                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+---------------+------------+---------------+------------+----------------+------------+-----------------+------------+---------------+------------+---------------+------------+----------------+------------+---------------+------------+---------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+-----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+-----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0          |7    |0    |61   |17   |18   |17   |30   |32   |466  |3     |11    |2     |17    |05db9164|89ddfee8|af7cfd7a|b423aeba|25c83c98|fbad5c96|407438c8|0b153874|a73ee510|f7276337|755e4a50|0f81738a|5978055e|051219e6|d5223973|4e17af52|27c07bd6|5bb2ec8e|064f1f80|a458ea53|15753ef8|ad3062eb|32c7478e|9dec18a0|f0f449dd|cb85773a|0.0         |(27,[0],[1.0]) |0.0         |(42,[0],[1.0]) |38.0        |(131,[38],[1.0])|64.0        |(126,[64],[1.0]) |0.0         |(12,[0],[1.0]) |1.0         |(4,[1],[1.0])  |1.0         |(104,[1],[1.0]) |0.0         |(19,[0],[1.0]) |0.0         |(2,[0],[1.0])  |14.0         |(110,[14],[1.0])|0.0          |(96,[0],[1.0])  |27.0         |(131,[27],[1.0])|0.0          |(89,[0],[1.0])  |3.0          |(10,[3],[1.0])  |2.0          |(98,[2],[1.0])  |98.0         |(131,[98],[1.0]) |3.0          |(5,[3],[1.0])   |0.0          |(66,[0],[1.0])  |8.0          |(48,[8],[1.0])  |0.0          |(3,[0],[1.0])   |110.0        |(131,[110],[1.0])|0.0          |(5,[0],[1.0])   |0.0          |(7,[0],[1.0])   |82.0         |(112,[82],[1.0])|2.0          |(16,[2],[1.0])  |73.0         |(118,[73],[1.0])|0.0  |(1656,[0,27,107,264,326,339,343,446,465,481,577,700,804,896,905,1099,1135,1137,1211,1251,1364,1385,1390,1479,1511,1598,1643,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,61.0,17.0,18.0,17.0,30.0,32.0,466.0,3.0,11.0,2.0,17.0])         |\n",
      "|0          |1    |327  |17   |0    |30   |0    |10   |3    |105  |1     |7     |0     |0     |5a9ed9b0|ef69887a|bbae32a2|fcc54ebe|25c83c98|7e0ccccf|2489e185|0b153874|a73ee510|3b08e48b|4d1f7d97|64da7308|954029f8|07d13a8f|b98be2c0|64f16754|e5ba7672|4bcc9449|aecf55cd|b1252a9d|c1c54b18|ad3062eb|3a171ecb|753ad37d|47907db5|2d3069f9|2.0         |(27,[2],[1.0]) |29.0        |(42,[29],[1.0])|77.0        |(131,[77],[1.0])|104.0       |(126,[104],[1.0])|0.0         |(12,[0],[1.0]) |0.0         |(4,[0],[1.0])  |31.0        |(104,[31],[1.0])|0.0         |(19,[0],[1.0]) |0.0         |(2,[0],[1.0])  |0.0          |(110,[0],[1.0]) |78.0         |(96,[78],[1.0]) |34.0         |(131,[34],[1.0])|84.0         |(89,[84],[1.0]) |1.0          |(10,[1],[1.0])  |74.0         |(98,[74],[1.0]) |29.0         |(131,[29],[1.0]) |0.0          |(5,[0],[1.0])   |44.0         |(66,[44],[1.0]) |33.0         |(48,[33],[1.0]) |1.0          |(3,[1],[1.0])   |37.0         |(131,[37],[1.0]) |0.0          |(5,[0],[1.0])   |1.0          |(7,[1],[1.0])   |69.0         |(112,[69],[1.0])|14.0         |(16,[14],[1.0]) |63.0         |(118,[63],[1.0])|0.0  |(1656,[2,56,146,304,326,338,373,446,465,467,655,707,888,894,977,1030,1132,1181,1236,1252,1291,1385,1391,1466,1523,1588,1643,1644,1645,1647,1649,1650,1651,1652,1653],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,327.0,17.0,30.0,10.0,3.0,105.0,1.0,7.0])                                       |\n",
      "|0          |0    |114  |4    |6    |5    |36   |22   |14   |394  |0     |8     |0     |2     |5bfa8ab5|287130e0|31c3612f|14f195ab|4cf72387|fbad5c96|6855ef53|0b153874|a73ee510|408fa381|b7094596|60e03064|1f9d2c38|1adce6ef|310d155b|dff2640e|3486227d|891589e7|21ddcdc9|a458ea53|7f311475|c9d4222a|32c7478e|3fdb382b|ea9a246c|49d68486|5.0         |(27,[5],[1.0]) |2.0         |(42,[2],[1.0]) |40.0        |(131,[40],[1.0])|111.0       |(126,[111],[1.0])|1.0         |(12,[1],[1.0]) |1.0         |(4,[1],[1.0])  |5.0         |(104,[5],[1.0]) |0.0         |(19,[0],[1.0]) |0.0         |(2,[0],[1.0])  |50.0         |(110,[50],[1.0])|1.0          |(96,[1],[1.0])  |58.0         |(131,[58],[1.0])|2.0          |(89,[2],[1.0])  |2.0          |(10,[2],[1.0])  |12.0         |(98,[12],[1.0]) |16.0         |(131,[16],[1.0]) |2.0          |(5,[2],[1.0])   |1.0          |(66,[1],[1.0])  |0.0          |(48,[0],[1.0])  |0.0          |(3,[0],[1.0])   |17.0         |(131,[17],[1.0]) |1.0          |(5,[1],[1.0])   |0.0          |(7,[0],[1.0])   |0.0          |(112,[0],[1.0]) |1.0          |(16,[1],[1.0])  |0.0          |(118,[0],[1.0]) |0.0  |(1656,[5,29,109,311,327,339,347,446,465,517,578,731,806,895,915,1017,1134,1138,1203,1251,1271,1386,1390,1397,1510,1525,1644,1645,1646,1647,1648,1649,1650,1651,1653,1655],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,114.0,4.0,6.0,5.0,36.0,22.0,14.0,394.0,8.0,2.0])                              |\n",
      "|0          |6    |88   |22   |32   |36   |58   |6    |25   |151  |1     |1     |1     |31    |7e5c2ff4|38d50e09|a889401d|d84e5d81|25c83c98|fbad5c96|dc7659bd|062b5529|a73ee510|efea433b|e51ddf94|2d72ffd1|3516f6e6|07d13a8f|f5fed91e|6049b763|27c07bd6|19fec6cc|21ddcdc9|a458ea53|a84fb99e|c9d4222a|3a171ecb|84edfefe|001f3601|c27f155b|25.0        |(27,[25],[1.0])|3.0         |(42,[3],[1.0]) |45.0        |(131,[45],[1.0])|29.0        |(126,[29],[1.0]) |0.0         |(12,[0],[1.0]) |1.0         |(4,[1],[1.0])  |48.0        |(104,[48],[1.0])|4.0         |(19,[4],[1.0]) |0.0         |(2,[0],[1.0])  |3.0          |(110,[3],[1.0]) |91.0         |(96,[91],[1.0]) |40.0         |(131,[40],[1.0])|13.0         |(89,[13],[1.0]) |1.0          |(10,[1],[1.0])  |35.0         |(98,[35],[1.0]) |101.0        |(131,[101],[1.0])|3.0          |(5,[3],[1.0])   |34.0         |(66,[34],[1.0]) |0.0          |(48,[0],[1.0])  |0.0          |(3,[0],[1.0])   |95.0         |(131,[95],[1.0]) |1.0          |(5,[1],[1.0])   |1.0          |(7,[1],[1.0])   |41.0         |(112,[41],[1.0])|0.0          |(16,[0],[1.0])  |15.0         |(118,[15],[1.0])|0.0  |(1656,[25,30,114,229,326,339,390,450,465,470,668,713,817,894,938,1102,1135,1171,1203,1251,1349,1386,1391,1438,1509,1540,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,88.0,22.0,32.0,36.0,58.0,6.0,25.0,151.0,1.0,1.0,1.0,31.0])|\n",
      "|0          |8    |13   |10   |19   |754  |37   |8    |20   |23   |1     |1     |0     |19    |8cf07265|d4be07ad|33ac1e20|36f6f194|25c83c98|7e0ccccf|01c31e6c|062b5529|a73ee510|5cad6330|2bcfb78f|3bbec69d|e6fc496d|b28479f6|98fca9df|46973e83|e5ba7672|cbae5931|3014a4b1|a458ea53|6f09e55c|ad3062eb|32c7478e|b2f178a3|001f3601|938732a0|3.0         |(27,[3],[1.0]) |40.0        |(42,[40],[1.0])|28.0        |(131,[28],[1.0])|32.0        |(126,[32],[1.0]) |0.0         |(12,[0],[1.0]) |0.0         |(4,[0],[1.0])  |11.0        |(104,[11],[1.0])|4.0         |(19,[4],[1.0]) |0.0         |(2,[0],[1.0])  |79.0         |(110,[79],[1.0])|7.0          |(96,[7],[1.0])  |76.0         |(131,[76],[1.0])|8.0          |(89,[8],[1.0])  |0.0          |(10,[0],[1.0])  |86.0         |(98,[86],[1.0]) |50.0         |(131,[50],[1.0]) |0.0          |(5,[0],[1.0])   |46.0         |(66,[46],[1.0]) |4.0          |(48,[4],[1.0])  |0.0          |(3,[0],[1.0])   |14.0         |(131,[14],[1.0]) |0.0          |(5,[0],[1.0])   |0.0          |(7,[0],[1.0])   |14.0         |(112,[14],[1.0])|0.0          |(16,[0],[1.0])  |33.0         |(118,[33],[1.0])|0.0  |(1656,[3,67,97,232,326,338,353,450,465,546,584,749,812,893,989,1051,1132,1183,1207,1251,1268,1385,1390,1411,1509,1558,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1655],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,13.0,10.0,19.0,754.0,37.0,8.0,20.0,23.0,1.0,1.0,19.0])           |\n",
      "+-----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+---------------+------------+---------------+------------+----------------+------------+-----------------+------------+---------------+------------+---------------+------------+----------------+------------+---------------+------------+---------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+-----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+-----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-------------+----------------+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainOHEdf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43048.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3250.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3250.0 (TID 228849, cluster-w261-w-6.c.w266-203603.internal, executor 16): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-95c6a3a283b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o43048.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3250.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3250.0 (TID 228849, cluster-w261-w-6.c.w266-203603.internal, executor 16): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int_1',\n",
       " 'int_2',\n",
       " 'int_3',\n",
       " 'int_4',\n",
       " 'int_5',\n",
       " 'int_6',\n",
       " 'int_7',\n",
       " 'int_8',\n",
       " 'int_9',\n",
       " 'int_10',\n",
       " 'int_11',\n",
       " 'int_12',\n",
       " 'int_13',\n",
       " 'categ_1',\n",
       " 'categ_2',\n",
       " 'categ_3',\n",
       " 'categ_4',\n",
       " 'categ_5',\n",
       " 'categ_6',\n",
       " 'categ_7',\n",
       " 'categ_8',\n",
       " 'categ_9',\n",
       " 'categ_10',\n",
       " 'categ_11',\n",
       " 'categ_12',\n",
       " 'categ_13',\n",
       " 'categ_14',\n",
       " 'categ_15',\n",
       " 'categ_16',\n",
       " 'categ_17',\n",
       " 'categ_18',\n",
       " 'categ_19',\n",
       " 'categ_20',\n",
       " 'categ_21',\n",
       " 'categ_22',\n",
       " 'categ_23',\n",
       " 'categ_24',\n",
       " 'categ_25',\n",
       " 'categ_26',\n",
       " 'categ_1Index',\n",
       " 'categ_1classVec',\n",
       " 'categ_2Index',\n",
       " 'categ_2classVec',\n",
       " 'categ_3Index',\n",
       " 'categ_3classVec',\n",
       " 'categ_4Index',\n",
       " 'categ_4classVec',\n",
       " 'categ_5Index',\n",
       " 'categ_5classVec',\n",
       " 'categ_6Index',\n",
       " 'categ_6classVec',\n",
       " 'categ_7Index',\n",
       " 'categ_7classVec',\n",
       " 'categ_8Index',\n",
       " 'categ_8classVec',\n",
       " 'categ_9Index',\n",
       " 'categ_9classVec',\n",
       " 'categ_10Index',\n",
       " 'categ_10classVec',\n",
       " 'categ_11Index',\n",
       " 'categ_11classVec',\n",
       " 'categ_12Index',\n",
       " 'categ_12classVec',\n",
       " 'categ_13Index',\n",
       " 'categ_13classVec',\n",
       " 'categ_14Index',\n",
       " 'categ_14classVec',\n",
       " 'categ_15Index',\n",
       " 'categ_15classVec',\n",
       " 'categ_16Index',\n",
       " 'categ_16classVec',\n",
       " 'categ_17Index',\n",
       " 'categ_17classVec',\n",
       " 'categ_18Index',\n",
       " 'categ_18classVec',\n",
       " 'categ_19Index',\n",
       " 'categ_19classVec',\n",
       " 'categ_20Index',\n",
       " 'categ_20classVec',\n",
       " 'categ_21Index',\n",
       " 'categ_21classVec',\n",
       " 'categ_22Index',\n",
       " 'categ_22classVec',\n",
       " 'categ_23Index',\n",
       " 'categ_23classVec',\n",
       " 'categ_24Index',\n",
       " 'categ_24classVec',\n",
       " 'categ_25Index',\n",
       " 'categ_25classVec',\n",
       " 'categ_26Index',\n",
       " 'categ_26classVec',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37672.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3091.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3091.0 (TID 219957, cluster-w261-w-6.c.w266-203603.internal, executor 16): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-68462c32ec3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o37672.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3091.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3091.0 (TID 219957, cluster-w261-w-6.c.w266-203603.internal, executor 16): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (vector) => vector)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 407, y.size = 1359\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:998)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:997)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1154)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:927)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "predictions.select(['probability']).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[int_1: int, categ_1: string, categ_2: string, categ_9: string]\n"
     ]
    }
   ],
   "source": [
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\n",
    "selected = predictions.select([\"int_1\", \"categ_1\", \"categ_2\", \"categ_9\"])\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6246435805602668\n",
      "0.06516424656674812\n",
      "0.03360207136478358\n",
      "0.01957540415658673\n",
      "0.009286072224274078\n",
      "0.004150578329405111\n",
      "0.0025272688370632226\n",
      "0.0012222793897761512\n",
      "0.0006214570303035642\n",
      "0.00029553695259758653\n",
      "0.00015151508917908565\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-f03ebcb52390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create 5-fold CrossValidator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossValidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimatorParamMaps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparamGrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run cross validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSubsetOHEdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"
     ]
    }
   ],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainSubsetOHEdf)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#ROC curve\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testsubsetDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"label\" does not exist.\\nAvailable fields: features, int_1, int_2, int_3, int_4, int_5, int_6, int_7, int_8, int_9, int_10, int_11, int_12, int_13, categ_1, categ_2, categ_3, categ_4, categ_5, categ_6, categ_7, categ_8, categ_9, categ_10, categ_11, categ_12, categ_13, categ_14, categ_15, categ_16, categ_17, categ_18, categ_19, categ_20, categ_21, categ_22, categ_23, categ_24, categ_25, categ_26, rawPrediction, probability, prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14505.evaluate.\n: java.lang.IllegalArgumentException: Field \"label\" does not exist.\nAvailable fields: features, int_1, int_2, int_3, int_4, int_5, int_6, int_7, int_8, int_9, int_10, int_11, int_12, int_13, categ_1, categ_2, categ_3, categ_4, categ_5, categ_6, categ_7, categ_8, categ_9, categ_10, categ_11, categ_12, categ_13, categ_14, categ_15, categ_16, categ_17, categ_18, categ_19, categ_20, categ_21, categ_22, categ_23, categ_24, categ_25, categ_26, rawPrediction, probability, prediction\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:267)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:267)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:266)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:71)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-69297f0791e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Area Under ROC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"label\" does not exist.\\nAvailable fields: features, int_1, int_2, int_3, int_4, int_5, int_6, int_7, int_8, int_9, int_10, int_11, int_12, int_13, categ_1, categ_2, categ_3, categ_4, categ_5, categ_6, categ_7, categ_8, categ_9, categ_10, categ_11, categ_12, categ_13, categ_14, categ_15, categ_16, categ_17, categ_18, categ_19, categ_20, categ_21, categ_22, categ_23, categ_24, categ_25, categ_26, rawPrediction, probability, prediction'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking stats for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some of the data in histograms\n",
    "# def plot_hist(hist_list):\n",
    "#     pd.DataFrame(\n",
    "#         list(zip(*hist_list)), \n",
    "#         columns=['bin', 'frequency']\n",
    "#     ).set_index(\n",
    "#         'bin'\n",
    "#     ).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp_hist = trainDF.select('int_1_log').rdd.flatMap(lambda x: x).histogram(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(temp_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tempDF = trainDF.select([\"int_1\"]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "# fig.set_size_inches(5, 5)\n",
    "# hist(axes, x=tempDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}