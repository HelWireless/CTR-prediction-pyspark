{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --upgrade networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade pyspark\n",
    "!pip install --upgrade pyspark_dist_explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display, HTML, display_html #usefull to display wide tables\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram\n",
    "from pyspark.sql import functions as F, types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.eventLog.dir', 'hdfs://cluster-w261-m/user/spark/eventlog'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://cluster-w261-m:8088/proxy/application_1543797393506_0002'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://cluster-w261-m.c.w266-203603.internal:4040'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'cluster-w261-m'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1543797393506_0002'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.historyServer.address', 'cluster-w261-m:18080'),\n",
       " ('spark.driver.maxResultSize', '3840m'),\n",
       " ('spark.driver.port', '34603'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'hdfs://cluster-w261-m/user/spark/eventlog'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.app.id', 'application_1543797393506_0002'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.memory', '11171m'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance'),\n",
       " ('spark.driver.host', 'cluster-w261-m.c.w266-203603.internal'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.executorEnv.PYTHONHASHSEED', '0'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.sql.parquet.cacheMetadata', 'false'),\n",
       " ('spark.driver.memory', '7680m'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.cbo.enabled', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\n",
    "\n",
    "#The number of cores can be specified with the --executor-cores flag when invoking \n",
    "#spark-submit, spark-shell, and pyspark from the command line, \n",
    "#or by setting the spark.executor.cores property in the spark-defaults.conf file \n",
    "#or on a SparkConf object.\n",
    "\n",
    "#The cores property controls the number of concurrent tasks an executor can run. \n",
    "#--executor-cores 5 means that each executor can run a maximum of five tasks at the same time.\n",
    "\n",
    "#The heap size can be controlled with the --executor-memory flag \n",
    "#or the spark.executor.memory property\n",
    "#The memory property impacts the amount of data Spark can cache, \n",
    "#as well as the maximum sizes of the shuffle data structures used for grouping, aggregations, and joins.\n",
    "\n",
    "#The --num-executors command-line flag or spark.executor.instances configuration property \n",
    "#control the number of executors requested. Starting in CDH 5.4/Spark 1.3, \n",
    "#you will be able to avoid setting this property by turning on dynamic allocation with the spark.dynamicAllocation.enabled property. Dynamic allocation enables a Spark application to request executors \n",
    "#when there is a backlog of pending tasks and free up executors when idle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.executor.memory\", '19g')\n",
    "# spark.conf.set('spark.executor.cores', '5')\n",
    "# sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_calc_stats(data, column):\n",
    "        return data.agg(F.avg(data[column]), F.min(data[column]), F.max(data[column]), \\\n",
    "                        F.stddev_pop(data[column]),F.var_pop(data[column]),F.skewness(data[column]) \\\n",
    "                       ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_check_null(data, column):\n",
    "    return data.filter( (data[column] ==\"\") |F.isnull(data[column])|F.isnan(data[column])\n",
    "                      ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display_stats(data):\n",
    "    dict1={}\n",
    "    countTotal = data.count()\n",
    "    for colname in [item[0] for item in data.dtypes if item[1].startswith('int')]:\n",
    "        list1=f_calc_stats(data,colname)\n",
    "        mean_val, min_val,max_val,stddev,var, skewness =list1[0]\n",
    "        count_nulls = f_check_null(data,colname)\n",
    "        dict1[colname]={}\n",
    "        dict1[colname]['mean'] = str(round(mean_val,2))\n",
    "        dict1[colname]['min'] = str(min_val)\n",
    "        dict1[colname]['max'] = str(max_val)\n",
    "        dict1[colname]['stddev'] = str(round(stddev,2))\n",
    "        dict1[colname]['var'] = str(round(var,2))\n",
    "        dict1[colname]['skewness'] = str(round(skewness,2))\n",
    "        dict1[colname]['nulls_nans'] = str(count_nulls)\n",
    "        dict1[colname]['pct_nulls_nans'] = str(round(float(count_nulls/countTotal*100),2))\n",
    "    display(HTML(pd.DataFrame(dict1).T.to_html( )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD = sc.textFile('gs://bucket-w261-final/data/train.txt',50)\n",
    "trainRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainRDD.map(lambda x: x.split('\\t')).toDF([\"clicked_0_1\", \"int_1\", \"int_2\", \"int_3\", \"int_4\", \"int_5\", \"int_6\", \"int_7\", \"int_8\", \"int_9\", \"int_10\", \"int_11\", \"int_12\", \"int_13\", \"categ_1\", \"categ_2\", \"categ_3\", \"categ_4\", \"categ_5\", \"categ_6\", \"categ_7\", \"categ_8\", \"categ_9\", \"categ_10\", \"categ_11\", \"categ_12\", \"categ_13\", \"categ_14\", \"categ_15\", \"categ_16\", \"categ_17\", \"categ_18\", \"categ_19\", \"categ_20\", \"categ_21\", \"categ_22\", \"categ_23\", \"categ_24\", \"categ_25\", \"categ_26\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.withColumn(\"int_1\", trainDF[\"int_1\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_2\", trainDF[\"int_2\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_3\", trainDF[\"int_3\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_4\", trainDF[\"int_4\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_5\", trainDF[\"int_5\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_6\", trainDF[\"int_6\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_7\", trainDF[\"int_7\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_8\", trainDF[\"int_8\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_9\", trainDF[\"int_9\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_10\", trainDF[\"int_10\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_11\", trainDF[\"int_11\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_12\", trainDF[\"int_12\"].cast(types.IntegerType()))\n",
    "trainDF = trainDF.withColumn(\"int_13\", trainDF[\"int_13\"].cast(types.IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# trainDF.groupby('int_1').count().sort(\"count\",ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>nulls_nans</th>\n",
       "      <th>pct_nulls_nans</th>\n",
       "      <th>skewness</th>\n",
       "      <th>stddev</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>int_1</th>\n",
       "      <td>5775</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>20793556</td>\n",
       "      <td>45.36</td>\n",
       "      <td>27.88</td>\n",
       "      <td>9.43</td>\n",
       "      <td>88.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_2</th>\n",
       "      <td>257675</td>\n",
       "      <td>105.85</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>391.46</td>\n",
       "      <td>153239.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_3</th>\n",
       "      <td>65535</td>\n",
       "      <td>26.91</td>\n",
       "      <td>0</td>\n",
       "      <td>9839447</td>\n",
       "      <td>21.46</td>\n",
       "      <td>81.49</td>\n",
       "      <td>397.97</td>\n",
       "      <td>158382.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_4</th>\n",
       "      <td>969</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0</td>\n",
       "      <td>9937369</td>\n",
       "      <td>21.68</td>\n",
       "      <td>4.09</td>\n",
       "      <td>8.79</td>\n",
       "      <td>77.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_5</th>\n",
       "      <td>23159456</td>\n",
       "      <td>18538.99</td>\n",
       "      <td>0</td>\n",
       "      <td>1183117</td>\n",
       "      <td>2.58</td>\n",
       "      <td>10.1</td>\n",
       "      <td>69394.6</td>\n",
       "      <td>4815610657.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_6</th>\n",
       "      <td>431037</td>\n",
       "      <td>116.06</td>\n",
       "      <td>0</td>\n",
       "      <td>10252328</td>\n",
       "      <td>22.37</td>\n",
       "      <td>184.98</td>\n",
       "      <td>382.57</td>\n",
       "      <td>146357.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_7</th>\n",
       "      <td>56311</td>\n",
       "      <td>16.33</td>\n",
       "      <td>0</td>\n",
       "      <td>1982866</td>\n",
       "      <td>4.33</td>\n",
       "      <td>46.39</td>\n",
       "      <td>66.05</td>\n",
       "      <td>4362.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_8</th>\n",
       "      <td>6047</td>\n",
       "      <td>12.52</td>\n",
       "      <td>0</td>\n",
       "      <td>22773</td>\n",
       "      <td>0.05</td>\n",
       "      <td>66.16</td>\n",
       "      <td>16.69</td>\n",
       "      <td>278.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_9</th>\n",
       "      <td>29019</td>\n",
       "      <td>106.11</td>\n",
       "      <td>0</td>\n",
       "      <td>1982866</td>\n",
       "      <td>4.33</td>\n",
       "      <td>8.52</td>\n",
       "      <td>220.28</td>\n",
       "      <td>48524.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>20793556</td>\n",
       "      <td>45.36</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_11</th>\n",
       "      <td>231</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0</td>\n",
       "      <td>1982866</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.2</td>\n",
       "      <td>27.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_12</th>\n",
       "      <td>4008</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "      <td>35071652</td>\n",
       "      <td>76.51</td>\n",
       "      <td>95.26</td>\n",
       "      <td>5.6</td>\n",
       "      <td>31.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_13</th>\n",
       "      <td>7393</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0</td>\n",
       "      <td>9937369</td>\n",
       "      <td>21.68</td>\n",
       "      <td>105.35</td>\n",
       "      <td>16.21</td>\n",
       "      <td>262.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_display_stats(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking stats for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some of the data in histograms\n",
    "# def plot_hist(hist_list):\n",
    "#     pd.DataFrame(\n",
    "#         list(zip(*hist_list)), \n",
    "#         columns=['bin', 'frequency']\n",
    "#     ).set_index(\n",
    "#         'bin'\n",
    "#     ).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp_hist = trainDF.select('int_1_log').rdd.flatMap(lambda x: x).histogram(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(temp_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tempDF = trainDF.select([\"int_1\"]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "# fig.set_size_inches(5, 5)\n",
    "# hist(axes, x=tempDF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}