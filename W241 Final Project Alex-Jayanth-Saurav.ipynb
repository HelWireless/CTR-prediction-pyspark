{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference :\n",
    "#RandomForest\n",
    "#https://github.com/apache/spark/blob/v2.2.0/mllib/src/main/scala/org/apache/spark/ml/classification/RandomForestClassifier.scala#L120\n",
    "#One hot encoding\n",
    "#https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "#LR from scratch\n",
    "#https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n",
    "#https://www.programcreek.com/python/example/106727/pyspark.ml.feature.StringIndexer\n",
    "\n",
    "#pyspark.mllib vs pyspark.ml\n",
    "#https://stackoverflow.com/questions/41074182/cannot-convert-type-class-pyspark-ml-linalg-sparsevector-into-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 19.8MB/s ta 0:00:01\n",
      "\u001b[31mpyspark 2.3.1 requires py4j==0.10.7, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-18.1\n",
      "Requirement already up-to-date: pandas in /opt/conda/lib/python3.6/site-packages (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "Collecting google-api-python-client\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/80/e034392c8190876167b4323aa497a3745e183b49b22cd2079c7d2f36c776/google_api_python_client-1.7.6-py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 4.2MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting google-auth-httplib2>=0.0.3 (from google-api-python-client)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (1.11.0)\n",
      "Collecting google-auth>=1.4.1 (from google-api-python-client)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/62/8b9612b1055cfbecd577e252446fe5f939f6818d0b7ddc27bb872f233cd4/google_auth-1.6.1-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 9.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0 (from google-api-python-client)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/7d/9d5a640c4f8bf2c8b1afc015e9a9d8de32e13c9016dcc4b0ec03481fb396/uritemplate-3.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (0.12.0)\n",
      "Collecting cachetools>=2.0.0 (from google-auth>=1.4.1->google-api-python-client)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/7e/08cd3846bebeabb6b1cfc4af8aae649d90249b4aeed080bddb5297f1d73b/cachetools-3.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4)\n",
      "Installing collected packages: cachetools, google-auth, google-auth-httplib2, uritemplate, google-api-python-client\n",
      "Successfully installed cachetools-3.0.0 google-api-python-client-1.7.6 google-auth-1.6.1 google-auth-httplib2-0.0.3 uritemplate-3.0.0\n",
      "Collecting seaborn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 39.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.15.2 in /opt/conda/lib/python3.6/site-packages (from seaborn) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=1.4.3 in /opt/conda/lib/python3.6/site-packages (from seaborn) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.15.2->seaborn) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.15.2->seaborn) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.3->seaborn) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (39.2.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.9.0\n",
      "Collecting networkx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.7MB 22.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx) (4.3.0)\n",
      "Building wheels for collected packages: networkx\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
      "Successfully built networkx\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-2.2\n",
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.2.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Found existing installation: matplotlib 3.0.1\n",
      "    Uninstalling matplotlib-3.0.1:\n",
      "      Successfully uninstalled matplotlib-3.0.1\n",
      "Successfully installed matplotlib-3.0.2\n",
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/01/a37e827c2d80c6a754e40e99b9826d978b55254cc6c6672b5b08f2e18a7f/pyspark-2.4.0.tar.gz (213.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 213.4MB 183kB/s eta 0:00:01   15% |█████                           | 34.0MB 78.3MB/s eta 0:00:03    95% |██████████████████████████████▍ | 202.8MB 88.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K    100% |████████████████████████████████| 204kB 53.2MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/cd/54/c2/abfcc942eddeaa7101228ebd6127a30dbdf903c72db4235b23\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Found existing installation: pyspark 2.3.1\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.0\n",
      "Collecting pyspark_dist_explore\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/c2/2f19468300f7c9d25dc526f0c11779d87e1a7c07d999347cf71a13282c0b/pyspark_dist_explore-0.1.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (0.23.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from pyspark_dist_explore) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas->pyspark_dist_explore) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas->pyspark_dist_explore) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->pyspark_dist_explore) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->pyspark_dist_explore) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->pyspark_dist_explore) (39.2.0)\n",
      "Installing collected packages: pyspark-dist-explore\n",
      "Successfully installed pyspark-dist-explore-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade google-api-python-client\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --upgrade networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade pyspark\n",
    "!pip install --upgrade pyspark_dist_explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, display_html #usefull to display wide tables\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "#from pyspark.mllib.linalg import VectorUDT\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql import functions as F, types\n",
    "from pyspark.sql.functions import (explode, col)\n",
    "from pyspark.sql.functions import col, row_number, concat, lit\n",
    "from pyspark.sql.functions import trim\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, split\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.window import Window\n",
    "import ast\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-w261-m.c.w266-203603.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3deff4bf60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rawDF = sqlContext.read.text('Toy_Example_Data.csv').withColumnRenamed(\"value\", \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rawDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "raw_train_df, raw_validation_df, raw_test_df = toy_rawDF.randomSplit(weights, seed)\n",
    "\n",
    "# Cache and count the DataFrames\n",
    "n_train = raw_train_df.cache().count()\n",
    "n_val = raw_validation_df.cache().count()\n",
    "n_test = raw_test_df.cache().count()\n",
    "print(n_train, n_val, n_test, str(n_train + n_val + n_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_train_df = parse_raw_df(raw_train_df)\n",
    "print(parsed_train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (explode, col)\n",
    "num_categories = (parsed_train_df\n",
    "                    .select(explode('features').alias('features'))\n",
    "                    .distinct()\n",
    "                    .select(col('features').getField('_1').alias('featureNumber'))\n",
    "                    .groupBy('featureNumber')\n",
    "                    .sum()\n",
    "                    .orderBy('featureNumber')\n",
    "                    .collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr_ohe_dict = create_one_hot_dict(parsed_train_df)\n",
    "num_ctr_ohe_feats = len(ctr_ohe_dict)\n",
    "print(num_ctr_ohe_feats)\n",
    "ctr_ohe_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_dict_broadcast = sc.broadcast(ctr_ohe_dict)\n",
    "ohe_dict_udf =  ohe_udf_generator(ohe_dict_broadcast)\n",
    "ohe_train_df =  parsed_train_df.select(parsed_train_df.label, ohe_dict_udf(parsed_train_df.features).alias('features'))\n",
    "#ohe_train_df.show(1)                  \n",
    "\n",
    "print(ohe_train_df.count())\n",
    "print(ohe_train_df.show())\n",
    "print(ohe_train_df.take(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_rdd = ohe_train_df \\\n",
    "                     .rdd \\\n",
    "                     .cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDropOut = ohe_train_rdd.map(lambda x: x[0]).mean()\n",
    "varDropOut = ohe_train_rdd.map(lambda x: x[0]).variance()\n",
    "print(f\"Mean: {meanDropOut}\")\n",
    "print(f\"Variance: {varDropOut}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = np.append(meanDropOut, np.zeros(ohe_train_df.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        ############## YOUR CODE HERE #############\n",
    "        model = GDUpdate(trainRDD, model, learningRate)\n",
    "        training_loss = LogLoss(trainRDD, model) \n",
    "        test_loss = LogLoss(testRDD, model)\n",
    "        ############## (END) YOUR CODE #############\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Log loss')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute log loss.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (x[0], np.append([1.0], x[1])))\n",
    "    ################## YOUR CODE HERE ##################\n",
    "    loss = augmentedData.map(lambda x: (-x[0] * np.log(sigmoid(W.dot(x[1]))) - (1 - x[0]) * np.log(1 - sigmoid(W.dot(x[1])))) ).mean()\n",
    "    ################## (END) YOUR CODE ##################\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: ( x[0], np.append([1.0], x[1]))).cache()\n",
    "    \n",
    "    ################## YOUR CODE HERE ################# \n",
    "    grad = augmentedData.map(lambda x: (sigmoid(W.dot(x[1])) - x[0])*x[1]).mean()\n",
    "    new_model = W - learningRate * grad\n",
    "    ################## (END) YOUR CODE ################# \n",
    "    \n",
    "    return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 5\n",
    "model = BASELINE\n",
    "print(f\"BASELINE:  Loss = {LogLoss(ohe_train_rdd,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(ohe_train_rdd, model)\n",
    "    loss = LogLoss(ohe_train_rdd, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_calc_stats(data, column):\n",
    "        return data.agg(F.avg(data[column]), F.min(data[column]), F.max(data[column]), \\\n",
    "                        F.stddev_pop(data[column]),F.var_pop(data[column]),F.skewness(data[column]) \\\n",
    "                       ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_check_null(data, column):\n",
    "    return data.filter( (data[column] ==\"\") |F.isnull(data[column])|F.isnan(data[column])\n",
    "                      ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking cardinality\n",
    "def f_display_stats_categ(df, inColList):\n",
    "    dict1={}\n",
    "    for col in inColList:\n",
    "        cardinal_cnt = df.select([col]).distinct().count()\n",
    "        dict1[col]={\"Count_Unique_Vals\":cardinal_cnt}\n",
    "        dict1[col]['Count_Empty_String'] = str(df.filter(df[col] == \"\").count())\n",
    "\n",
    "    display(HTML(pd.DataFrame(dict1).T.to_html( )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display_stats_int(data):\n",
    "    dict1={}\n",
    "    countTotal = data.count()\n",
    "    for colname in [item[0] for item in data.dtypes if item[1].startswith('double')]:\n",
    "        list1=f_calc_stats(data,colname)\n",
    "        mean_val, min_val,max_val,stddev,var, skewness =list1[0]\n",
    "        count_nulls = f_check_null(data,colname)\n",
    "        dict1[colname]={}\n",
    "        dict1[colname]['mean'] = str(round(mean_val,2))\n",
    "        dict1[colname]['min'] = str(min_val)\n",
    "        dict1[colname]['max'] = str(max_val)\n",
    "        dict1[colname]['stddev'] = str(round(stddev,2))\n",
    "        dict1[colname]['var'] = str(round(var,2))\n",
    "        dict1[colname]['skewness'] = str(round(skewness,2))\n",
    "        dict1[colname]['nulls_nans'] = str(count_nulls)\n",
    "        dict1[colname]['pct_nulls_nans'] = str(round(float(count_nulls/countTotal*100),2))\n",
    "        dict1[colname]['count_empty_string'] = str(data.filter(data[colname] == \"\").count())\n",
    "        dict1[colname]['count_unique_values'] = str(data.select([colname]).distinct().count())\n",
    "        dict1[colname]['count'] = 'N/A'\n",
    "    dict1['TOTAL']={}\n",
    "    dict1['TOTAL']['count'] = str(data.count())\n",
    "   #Transposing dataframe to keep column names as rows\n",
    "    display(HTML(pd.DataFrame(dict1).T.to_html( )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_display_corr(df, int_col_list):\n",
    "    sampleDF=df.sample(seed=1, fraction=0.5, withReplacement=False)\n",
    "    dict1={}\n",
    "    for col in int_col_list:\n",
    "        corr = trainWithColsDF.stat.corr('label',col)\n",
    "        cov = trainWithColsDF.stat.cov('label',col)\n",
    "        dict1[col]={}\n",
    "        dict1[col][\"Corr\"]=corr\n",
    "        dict1[col][\"Cov\"]=cov\n",
    "    display(HTML(pd.DataFrame(dict1).T.to_html( )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from RDD\n",
    "def f_covert_to_df(rdd, col_list):\n",
    "    return rdd.map(lambda x: x.split('\\t')).toDF(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert integer columns to IntegerType from String\n",
    "def f_cast_str_to_int(df, integer_col_list):\n",
    "    for col in integer_col_list:\n",
    "        df = df.withColumn(col, df[col].cast(types.IntegerType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows where at least one column has empty string\n",
    "def f_remove_empty_string(df, categ_col_list):\n",
    "    for categ_col in categ_col_list:\n",
    "        df = df.filter(df[categ_col] != \"\")\n",
    "        #df = df.filter(col(categ_col) != \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColList=[\"clicked_0_1\"]\n",
    "intColList=[\"int_1\", \"int_2\", \"int_3\", \"int_4\", \"int_5\", \"int_6\", \"int_7\", \"int_8\", \"int_9\", \"int_10\", \"int_11\", \"int_12\", \"int_13\"]\n",
    "categColList=[\"categ_1\", \"categ_2\", \"categ_3\", \"categ_4\", \"categ_5\", \"categ_6\", \"categ_7\", \"categ_8\", \"categ_9\", \"categ_10\", \"categ_11\", \"categ_12\", \"categ_13\", \"categ_14\", \"categ_15\", \"categ_16\", \"categ_17\", \"categ_18\", \"categ_19\", \"categ_20\", \"categ_21\", \"categ_22\", \"categ_23\", \"categ_24\", \"categ_25\", \"categ_26\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_point(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    values = point.split('\\t')[1:]\n",
    "    #values = filter(None, values)\n",
    "    indices = range(len(values))\n",
    "    return zip(indices,values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),StructField('_2', StringType())])))\n",
    "\n",
    "def parse_raw_df(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and feature.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'feature' columns.   \n",
    "  \n",
    "    \"\"\"\n",
    "    return (raw_df.select(split(raw_df.text,'\\t').getItem(0).cast(\"double\").alias('label'),\n",
    "                         parse_point_udf(raw_df.text).alias('features'))\n",
    "                        .cache())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = sqlContext.read.text('gs://bucket-w261-final/data/train.txt').withColumnRenamed(\"value\", \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 1\n",
    "\n",
    "# Use randomSplit with weights and seed\n",
    "rawTrainDF, rawValidationDF, rawTestDF = rawDF.randomSplit(weights, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25% sampling due to memory errors\n",
    "rawTrainNewDF = rawTrainDF.sample(withReplacement=False, fraction=0.25, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawTrainNewDF.cache()\n",
    "rawValidationDF.cache()\n",
    "rawTestDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9167896"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawTrainNewDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawTrainNewDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainWithColsDF=rawTrainNewDF.withColumn('tmp', split('text', '\\t')).select( col(\"tmp\").getItem(0).cast(\"double\").alias(\"label\")\n",
    "                                                                            ,col(\"tmp\").getItem(1).cast(\"double\").alias(\"int_1\")\n",
    "                                                                            ,col(\"tmp\").getItem(2).cast(\"double\").alias(\"int_2\")\n",
    "                                                                            ,col(\"tmp\").getItem(3).cast(\"double\").alias(\"int_3\")\n",
    "                                                                            ,col(\"tmp\").getItem(4).cast(\"double\").alias(\"int_4\")\n",
    "                                                                            ,col(\"tmp\").getItem(5).cast(\"double\").alias(\"int_5\")\n",
    "                                                                            ,col(\"tmp\").getItem(6).cast(\"double\").alias(\"int_6\")\n",
    "                                                                            ,col(\"tmp\").getItem(7).cast(\"double\").alias(\"int_7\")\n",
    "                                                                            ,col(\"tmp\").getItem(8).cast(\"double\").alias(\"int_8\")\n",
    "                                                                            ,col(\"tmp\").getItem(9).cast(\"double\").alias(\"int_9\")\n",
    "                                                                            ,col(\"tmp\").getItem(10).cast(\"double\").alias(\"int_10\")\n",
    "                                                                            ,col(\"tmp\").getItem(11).cast(\"double\").alias(\"int_11\")\n",
    "                                                                            ,col(\"tmp\").getItem(12).cast(\"double\").alias(\"int_12\")\n",
    "                                                                            ,col(\"tmp\").getItem(13).cast(\"double\").alias(\"int_13\")\n",
    "                                                                            ,col(\"tmp\").getItem(14).alias(\"categ_1\")\n",
    "                                                                            ,col(\"tmp\").getItem(15).alias(\"categ_2\")\n",
    "                                                                            ,col(\"tmp\").getItem(16).alias(\"categ_3\")\n",
    "                                                                            ,col(\"tmp\").getItem(17).alias(\"categ_4\")\n",
    "                                                                            ,col(\"tmp\").getItem(18).alias(\"categ_5\")\n",
    "                                                                            ,col(\"tmp\").getItem(19).alias(\"categ_6\")\n",
    "                                                                            ,col(\"tmp\").getItem(20).alias(\"categ_7\")\n",
    "                                                                            ,col(\"tmp\").getItem(21).alias(\"categ_8\")\n",
    "                                                                            ,col(\"tmp\").getItem(22).alias(\"categ_9\")\n",
    "                                                                            ,col(\"tmp\").getItem(23).alias(\"categ_10\")\n",
    "                                                                            ,col(\"tmp\").getItem(24).alias(\"categ_11\")\n",
    "                                                                            ,col(\"tmp\").getItem(25).alias(\"categ_12\")\n",
    "                                                                            ,col(\"tmp\").getItem(26).alias(\"categ_13\")\n",
    "                                                                            ,col(\"tmp\").getItem(27).alias(\"categ_14\")\n",
    "                                                                            ,col(\"tmp\").getItem(28).alias(\"categ_15\")\n",
    "                                                                            ,col(\"tmp\").getItem(29).alias(\"categ_16\")\n",
    "                                                                            ,col(\"tmp\").getItem(30).alias(\"categ_17\")\n",
    "                                                                            ,col(\"tmp\").getItem(31).alias(\"categ_18\")\n",
    "                                                                            ,col(\"tmp\").getItem(32).alias(\"categ_19\")\n",
    "                                                                            ,col(\"tmp\").getItem(33).alias(\"categ_20\")\n",
    "                                                                            ,col(\"tmp\").getItem(34).alias(\"categ_21\")\n",
    "                                                                            ,col(\"tmp\").getItem(35).alias(\"categ_22\")\n",
    "                                                                            ,col(\"tmp\").getItem(36).alias(\"categ_23\")\n",
    "                                                                            ,col(\"tmp\").getItem(37).alias(\"categ_24\")\n",
    "                                                                            ,col(\"tmp\").getItem(38).alias(\"categ_25\")\n",
    "                                                                            ,col(\"tmp\").getItem(39).alias(\"categ_26\")\n",
    "                                                            ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text='0\\t\\t-1\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t05db9164\\t38a947a1\\t\\t\\t25c83c98\\t\\t1d7560d9\\t64523cfa\\t7cc72ec2\\t3b08e48b\\t8951ed6a\\t\\t27dc8af3\\tb28479f6\\t4df46b2c\\t\\t2005abd1\\t40685634\\t\\t\\t\\t\\t32c7478e\\t\\t\\t')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawTrainNewDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, int_1=None, int_2=-1.0, int_3=None, int_4=None, int_5=None, int_6=None, int_7=None, int_8=None, int_9=None, int_10=None, int_11=None, int_12=None, int_13=None, categ_1='05db9164', categ_2='38a947a1', categ_3='', categ_4='', categ_5='25c83c98', categ_6='', categ_7='1d7560d9', categ_8='64523cfa', categ_9='7cc72ec2', categ_10='3b08e48b', categ_11='8951ed6a', categ_12='', categ_13='27dc8af3', categ_14='b28479f6', categ_15='4df46b2c', categ_16='', categ_17='2005abd1', categ_18='40685634', categ_19='', categ_20='', categ_21='', categ_22='', categ_23='32c7478e', categ_24='', categ_25='', categ_26='')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainWithColsDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelColList=[\"label\"]\n",
    "intColList=[\"int_1\", \"int_2\", \"int_3\", \"int_4\", \"int_5\", \"int_6\", \"int_7\", \"int_8\", \"int_9\", \"int_10\", \"int_11\", \"int_12\", \"int_13\"]\n",
    "categColList=[\"categ_1\", \"categ_2\", \"categ_3\", \"categ_4\", \"categ_5\", \"categ_6\", \"categ_7\", \"categ_8\", \"categ_9\", \"categ_10\", \"categ_11\", \"categ_12\", \"categ_13\", \"categ_14\", \"categ_15\", \"categ_16\", \"categ_17\", \"categ_18\", \"categ_19\", \"categ_20\", \"categ_21\", \"categ_22\", \"categ_23\", \"categ_24\", \"categ_25\", \"categ_26\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count_empty_string</th>\n",
       "      <th>count_unique_values</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>nulls_nans</th>\n",
       "      <th>pct_nulls_nans</th>\n",
       "      <th>skewness</th>\n",
       "      <th>stddev</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_1</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>456</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4159327</td>\n",
       "      <td>45.37</td>\n",
       "      <td>18.49</td>\n",
       "      <td>9.4</td>\n",
       "      <td>88.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_2</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>6850</td>\n",
       "      <td>35521.0</td>\n",
       "      <td>105.91</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.22</td>\n",
       "      <td>390.39</td>\n",
       "      <td>152401.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_3</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>5781</td>\n",
       "      <td>65535.0</td>\n",
       "      <td>26.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1966409</td>\n",
       "      <td>21.45</td>\n",
       "      <td>83.89</td>\n",
       "      <td>392.27</td>\n",
       "      <td>153878.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_4</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>877.0</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987342</td>\n",
       "      <td>21.68</td>\n",
       "      <td>3.94</td>\n",
       "      <td>8.77</td>\n",
       "      <td>76.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_5</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>256027</td>\n",
       "      <td>23159456.0</td>\n",
       "      <td>18563.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>236154</td>\n",
       "      <td>2.58</td>\n",
       "      <td>13.23</td>\n",
       "      <td>69907.29</td>\n",
       "      <td>4887029138.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_6</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>7358</td>\n",
       "      <td>430898.0</td>\n",
       "      <td>115.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2050660</td>\n",
       "      <td>22.37</td>\n",
       "      <td>284.12</td>\n",
       "      <td>411.87</td>\n",
       "      <td>169638.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_7</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>2727</td>\n",
       "      <td>34536.0</td>\n",
       "      <td>16.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>397046</td>\n",
       "      <td>4.33</td>\n",
       "      <td>48.8</td>\n",
       "      <td>67.18</td>\n",
       "      <td>4513.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_8</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>747</td>\n",
       "      <td>5664.0</td>\n",
       "      <td>12.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4476</td>\n",
       "      <td>0.05</td>\n",
       "      <td>66.97</td>\n",
       "      <td>16.63</td>\n",
       "      <td>276.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_9</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>5142</td>\n",
       "      <td>29019.0</td>\n",
       "      <td>106.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>397046</td>\n",
       "      <td>4.33</td>\n",
       "      <td>8.55</td>\n",
       "      <td>220.01</td>\n",
       "      <td>48405.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_10</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4159327</td>\n",
       "      <td>45.37</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_11</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>181.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>397046</td>\n",
       "      <td>4.33</td>\n",
       "      <td>6.05</td>\n",
       "      <td>5.21</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_12</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7013777</td>\n",
       "      <td>76.5</td>\n",
       "      <td>145.41</td>\n",
       "      <td>5.83</td>\n",
       "      <td>33.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_13</th>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>674</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987342</td>\n",
       "      <td>21.68</td>\n",
       "      <td>120.81</td>\n",
       "      <td>16.3</td>\n",
       "      <td>265.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>9167896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count_Empty_String</th>\n",
       "      <th>Count_Unique_Vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>categ_1</th>\n",
       "      <td>0</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_2</th>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_3</th>\n",
       "      <td>311243</td>\n",
       "      <td>2539621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_4</th>\n",
       "      <td>311243</td>\n",
       "      <td>701163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_5</th>\n",
       "      <td>0</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_6</th>\n",
       "      <td>1110211</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_7</th>\n",
       "      <td>0</td>\n",
       "      <td>12238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_8</th>\n",
       "      <td>0</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_9</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_10</th>\n",
       "      <td>0</td>\n",
       "      <td>64089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_11</th>\n",
       "      <td>0</td>\n",
       "      <td>5397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_12</th>\n",
       "      <td>311243</td>\n",
       "      <td>2175829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_13</th>\n",
       "      <td>0</td>\n",
       "      <td>3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_14</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_15</th>\n",
       "      <td>0</td>\n",
       "      <td>12973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_16</th>\n",
       "      <td>311243</td>\n",
       "      <td>1520386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_17</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_18</th>\n",
       "      <td>0</td>\n",
       "      <td>5061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_19</th>\n",
       "      <td>4034202</td>\n",
       "      <td>2107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_20</th>\n",
       "      <td>4034202</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_21</th>\n",
       "      <td>311243</td>\n",
       "      <td>1893572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_22</th>\n",
       "      <td>6990570</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_23</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_24</th>\n",
       "      <td>311243</td>\n",
       "      <td>135984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_25</th>\n",
       "      <td>4034202</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ_26</th>\n",
       "      <td>4034202</td>\n",
       "      <td>83237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation and Covariance w.r.t target field\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corr</th>\n",
       "      <th>Cov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>int_1</th>\n",
       "      <td>0.104506</td>\n",
       "      <td>0.326917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_2</th>\n",
       "      <td>0.044621</td>\n",
       "      <td>7.603534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_3</th>\n",
       "      <td>0.009372</td>\n",
       "      <td>1.422942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_4</th>\n",
       "      <td>-0.055757</td>\n",
       "      <td>-0.202735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_5</th>\n",
       "      <td>-0.076178</td>\n",
       "      <td>-2296.478111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_6</th>\n",
       "      <td>-0.051921</td>\n",
       "      <td>-8.296947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_7</th>\n",
       "      <td>0.083761</td>\n",
       "      <td>2.405540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_8</th>\n",
       "      <td>-0.027570</td>\n",
       "      <td>-0.200104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_9</th>\n",
       "      <td>0.024018</td>\n",
       "      <td>2.267424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_10</th>\n",
       "      <td>0.191403</td>\n",
       "      <td>0.049470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_11</th>\n",
       "      <td>0.158222</td>\n",
       "      <td>0.353766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_12</th>\n",
       "      <td>0.047553</td>\n",
       "      <td>0.059298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_13</th>\n",
       "      <td>-0.074932</td>\n",
       "      <td>-0.484548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_display_stats_int(trainWithColsDF)\n",
    "f_display_stats_categ(trainWithColsDF, categColList)\n",
    "print(\"Correlation and Covariance w.r.t target field\")\n",
    "f_display_corr(trainWithColsDF, intColList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://lncohn.com/spark/ctr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted, and that the\n",
    "        function handles missing features.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    indices = sorted([ohe_dict_broadcast.value[feat] for feat in raw_feats if feat in ohe_dict_broadcast.value])\n",
    "    values = np.ones(len(raw_feats))\n",
    "    return SparseVector(num_ohe_feats,indices,values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ohe_udf_generator(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "    length = len(ohe_dict_broadcast.value)\n",
    "    return udf(lambda x: one_hot_encoding(x, ohe_dict_broadcast, length), VectorUDT())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_dict(input_df):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n",
    "            (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    input_distinct_feats_df = input_df.select(explode(input_df.features)).distinct()\n",
    "    input_ohe_dict = (input_distinct_feats_df\n",
    "                     .rdd\n",
    "                     .map(lambda r: tuple(r[0]))\n",
    "                     .zipWithIndex().collectAsMap())\n",
    "    return input_ohe_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_train_df = parse_raw_df(rawTrainNewDF)\n",
    "parsed_validation_df = parse_raw_df(rawValidationDF)\n",
    "parsed_test_df = parse_raw_df(rawTestDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: array<struct<_1:bigint,_2:string>>]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_train_df.cache()\n",
    "parsed_validation_df.cache()\n",
    "parsed_test_df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9168574"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, features=[Row(_1=0, _2='1382'), Row(_1=1, _2='5'), Row(_1=2, _2='4'), Row(_1=3, _2='1'), Row(_1=4, _2='181'), Row(_1=5, _2='fb936136'), Row(_1=6, _2='37c9c164'), Row(_1=7, _2='07b5194c'), Row(_1=8, _2='891b62e7'), Row(_1=9, _2='7b4723c4'), Row(_1=10, _2='c5c50484'), Row(_1=11, _2='9727dd16')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = (parsed_train_df\n",
    "                    .select(explode('features').alias('features'))\n",
    "                    .distinct()\n",
    "                    .select(col('features').getField('_1').alias('featureNumber'))\n",
    "                    .groupBy('featureNumber')\n",
    "                    .sum()\n",
    "                    .orderBy('featureNumber')\n",
    "                    .collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(featureNumber=0, sum(featureNumber)=0),\n",
       " Row(featureNumber=1, sum(featureNumber)=6850),\n",
       " Row(featureNumber=2, sum(featureNumber)=11562),\n",
       " Row(featureNumber=3, sum(featureNumber)=876),\n",
       " Row(featureNumber=4, sum(featureNumber)=1024108),\n",
       " Row(featureNumber=5, sum(featureNumber)=36790),\n",
       " Row(featureNumber=6, sum(featureNumber)=16362),\n",
       " Row(featureNumber=7, sum(featureNumber)=5229),\n",
       " Row(featureNumber=8, sum(featureNumber)=41136),\n",
       " Row(featureNumber=9, sum(featureNumber)=99),\n",
       " Row(featureNumber=10, sum(featureNumber)=1470),\n",
       " Row(featureNumber=11, sum(featureNumber)=2915),\n",
       " Row(featureNumber=12, sum(featureNumber)=8088),\n",
       " Row(featureNumber=13, sum(featureNumber)=18954),\n",
       " Row(featureNumber=14, sum(featureNumber)=7868),\n",
       " Row(featureNumber=15, sum(featureNumber)=38094315),\n",
       " Row(featureNumber=16, sum(featureNumber)=11218608),\n",
       " Row(featureNumber=17, sum(featureNumber)=5185),\n",
       " Row(featureNumber=18, sum(featureNumber)=414),\n",
       " Row(featureNumber=19, sum(featureNumber)=232522),\n",
       " Row(featureNumber=20, sum(featureNumber)=12660),\n",
       " Row(featureNumber=21, sum(featureNumber)=63),\n",
       " Row(featureNumber=22, sum(featureNumber)=1409958),\n",
       " Row(featureNumber=23, sum(featureNumber)=124131),\n",
       " Row(featureNumber=24, sum(featureNumber)=52219896),\n",
       " Row(featureNumber=25, sum(featureNumber)=79600),\n",
       " Row(featureNumber=26, sum(featureNumber)=702),\n",
       " Row(featureNumber=27, sum(featureNumber)=350271),\n",
       " Row(featureNumber=28, sum(featureNumber)=42570808),\n",
       " Row(featureNumber=29, sum(featureNumber)=290),\n",
       " Row(featureNumber=30, sum(featureNumber)=151830),\n",
       " Row(featureNumber=31, sum(featureNumber)=65317),\n",
       " Row(featureNumber=32, sum(featureNumber)=128),\n",
       " Row(featureNumber=33, sum(featureNumber)=62487876),\n",
       " Row(featureNumber=34, sum(featureNumber)=612),\n",
       " Row(featureNumber=35, sum(featureNumber)=525),\n",
       " Row(featureNumber=36, sum(featureNumber)=4895424),\n",
       " Row(featureNumber=37, sum(featureNumber)=3552),\n",
       " Row(featureNumber=38, sum(featureNumber)=3163006)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an OHE dictionary from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9444472\n",
      "8499753\n"
     ]
    }
   ],
   "source": [
    "ctr_ohe_dict = create_one_hot_dict(parsed_train_df)\n",
    "num_ctr_ohe_feats = len(ctr_ohe_dict)\n",
    "print(num_ctr_ohe_feats)\n",
    "print(ctr_ohe_dict[(0, '')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply OHE to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_dict_broadcast = sc.broadcast(ctr_ohe_dict)\n",
    "ohe_dict_udf =  ohe_udf_generator(ohe_dict_broadcast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_train_df =  parsed_train_df.select(parsed_train_df.label, ohe_dict_udf(parsed_train_df.features).alias('features'))\n",
    "ohe_train_df.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Handling unseen features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_v2(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted, and that the\n",
    "        function handles missing features.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    indices = sorted([ohe_dict_broadcast.value[feat] for feat in raw_feats if feat in ohe_dict_broadcast.value])\n",
    "    values = np.ones(len([feat for feat in raw_feats if feat in ohe_dict_broadcast.value] ))\n",
    "#     values = np.ones(len([raw_feats]))\n",
    "    return SparseVector(num_ohe_feats,indices,values)\n",
    "\n",
    "def ohe_udf_generator_v2(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "    length = len(ohe_dict_broadcast.value)\n",
    "    return udf(lambda x: one_hot_encoding_v2(x, ohe_dict_broadcast, length), VectorUDT())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_dict_missing_udf = ohe_udf_generator_v2(ohe_dict_broadcast)\n",
    "\n",
    "ohe_test_df = parsed_test_df.select(parsed_test_df.label,  ohe_dict_missing_udf(parsed_test_df.features).alias('features')).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4585725\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(9444472,[282595,424288,659969,706928,1320318,1557022,1557024,1604081,2264580,2406696,2500892,2736370,3113587,3255788,4010909,4058045,4153235,4200705,4578630,4814588,4909212,5146054,5524541,5808565,5855851,6091432,6233661,6611261,6705776,7602305,7884943,8074256,8358485,8405758,8452489,8499753,9161299,9350284,9350285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ohe_test_df.count())\n",
    "print(ohe_test_df.show(1, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, features=SparseVector(9444472, {282595: 1.0, 424288: 1.0, 659969: 1.0, 706928: 1.0, 989772: 1.0, 1320318: 1.0, 1557022: 1.0, 1557023: 1.0, 1557024: 1.0, 2123207: 1.0, 2264580: 1.0, 2406696: 1.0, 2736370: 1.0, 3113587: 1.0, 3255788: 1.0, 4010909: 1.0, 4153235: 1.0, 4200705: 1.0, 4200706: 1.0, 4297751: 1.0, 4814588: 1.0, 4909212: 1.0, 5146054: 1.0, 5524541: 1.0, 5808565: 1.0, 5903591: 1.0, 6233661: 1.0, 6611261: 1.0, 6705776: 1.0, 6894897: 1.0, 7602305: 1.0, 8074256: 1.0, 8358485: 1.0, 8405758: 1.0, 8452489: 1.0, 8499753: 1.0, 9161299: 1.0, 9350284: 1.0, 9350285: 1.0}))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_train_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, features=SparseVector(9444472, {282595: 1.0, 424288: 1.0, 659969: 1.0, 706928: 1.0, 1320318: 1.0, 1557022: 1.0, 1557024: 1.0, 1604081: 1.0, 2264580: 1.0, 2406696: 1.0, 2500892: 1.0, 2736370: 1.0, 3113587: 1.0, 3255788: 1.0, 4010909: 1.0, 4058045: 1.0, 4153235: 1.0, 4200705: 1.0, 4578630: 1.0, 4814588: 1.0, 4909212: 1.0, 5146054: 1.0, 5524541: 1.0, 5808565: 1.0, 5855851: 1.0, 6091432: 1.0, 6233661: 1.0, 6611261: 1.0, 6705776: 1.0, 7602305: 1.0, 7884943: 1.0, 8074256: 1.0, 8358485: 1.0, 8405758: 1.0, 8452489: 1.0, 8499753: 1.0, 9161299: 1.0, 9350284: 1.0, 9350285: 1.0}))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_test_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CTR prediction and logloss evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: -1.0661652510744617\n",
      "length of coefficients: 9444472\n",
      "[-0.0015861844761605752, -0.0015861170660600307, -0.0015861170660600307, -0.0015861170660600307, -0.0015804214023579803]\n"
     ]
    }
   ],
   "source": [
    "standardization = False\n",
    "elastic_net_param = 0.0\n",
    "reg_param = .01\n",
    "max_iter = 5\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxIter=max_iter, regParam=reg_param, elasticNetParam=elastic_net_param, fitIntercept=True,  standardization=standardization)\n",
    "\n",
    "lr_model_basic = lr.fit(ohe_train_df)\n",
    "\n",
    "print('intercept: {0}'.format(lr_model_basic.intercept))\n",
    "print('length of coefficients: {0}'.format(len(lr_model_basic.coefficients)))\n",
    "sorted_coefficients = sorted(lr_model_basic.coefficients)[:5]\n",
    "print(sorted_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location=\"gs://bucket-w261-final/data/lr_model_basic.txt\"\n",
    "lr_model_basic.write().overwrite().save(save_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Log loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, log, col\n",
    "epsilon = 1e-16\n",
    "\n",
    "def add_log_loss(df):\n",
    "    \"\"\"Computes and adds a 'log_loss' column to a DataFrame using 'p' and 'label' columns.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we add a small value (epsilon) to it and when\n",
    "        p is 1 we subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'p' and 'label' columns): A DataFrame with a probability column\n",
    "            'p' and a 'label' column that corresponds to y in the log loss formula.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with an additional column called 'log_loss' where 'log_loss' column contains the loss value as explained above.\n",
    "        \n",
    "        if y == 1:\n",
    "          return -log(epsilon + p) if p == 0 else -log(p)\n",
    "        elif y == 0:\n",
    "          return -log(1 - p + epsilon) if p == 1 else -log(1 - p)\n",
    "          \n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    return (df.select(df['p'],df['label'],\n",
    "                when(df['label']==1.0,\n",
    "                     (when(df['p']!=0,-log(df['p'])))\n",
    "                     .otherwise(-log(epsilon + df['p'])))\n",
    "               .when(df['label']==0.0,\n",
    "                     (when(df['p']!=1.0,-log(1.0-df['p'])))\n",
    "                     .otherwise(-log(1.0 - df['p'] + epsilon)))\n",
    "               .alias(\"log_loss\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4c) Baseline log loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class one fraction = 0.256\n",
      "Baseline Train Logloss = 0.569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "class_one_frac_train = ohe_train_df.select(F.sum('label')).collect()[0][0] / ohe_train_df.count()\n",
    "\n",
    "ohe_train_df = (ohe_train_df.withColumn(\"p\", lit(class_one_frac_train)))\n",
    "print('Training class one fraction = {0:.3f}'.format(class_one_frac_train))\n",
    "\n",
    "log_loss_tr_base = (add_log_loss(ohe_train_df).select(F.sum('log_loss')).collect()[0][0] / ohe_train_df.count())\n",
    "print('Baseline Train Logloss = {0:.3f}\\n'.format(log_loss_tr_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Predicted probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from math import exp #  exp(-t) = e^-t\n",
    "\n",
    "def add_probability(df, model):\n",
    "    \"\"\"Adds a probability column ('p') to a DataFrame given a model\"\"\"\n",
    "    coefficients_broadcast = sc.broadcast(model.coefficients)\n",
    "    intercept = model.intercept\n",
    "\n",
    "    def get_p(features):\n",
    "        \"\"\"Calculate the probability for an observation given a list of features.\n",
    "\n",
    "        Note:\n",
    "            We'll bound our raw prediction between 20 and -20 for numerical purposes.\n",
    "\n",
    "        Args:\n",
    "            features: the features\n",
    "\n",
    "        Returns:\n",
    "            float: A probability between 0 and 1.\n",
    "        \"\"\"\n",
    "        # Compute the raw value\n",
    "        raw_prediction = features.dot(coefficients_broadcast.value)+intercept\n",
    "        # Bound the raw value between 20 and -20\n",
    "        raw_prediction = min(raw_prediction, 20)\n",
    "        raw_prediction = max(raw_prediction, -20)\n",
    "        return 1.0 / (1.0 + exp(-raw_prediction))\n",
    "\n",
    "    get_p_udf = udf(get_p, DoubleType())\n",
    "    return df.withColumn('p', get_p_udf('features'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|label|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |p                  |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|0.0  |(9444472,[282595,424288,659969,706928,989772,1320318,1557022,1557023,1557024,2123207,2264580,2406696,2736370,3113587,3255788,4010909,4153235,4200705,4200706,4297751,4814588,4909212,5146054,5524541,5808565,5903591,6233661,6611261,6705776,6894897,7602305,8074256,8358485,8405758,8452489,8499753,9161299,9350284,9350285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |0.2538831479776069 |\n",
      "|0.0  |(9444472,[282595,424288,659969,706928,1227610,1320318,1557022,2264580,2406696,2736370,3113587,3255788,3545339,4010909,4153235,4200705,4200706,4437057,4578630,4767546,4814588,4909212,5146054,5288216,5524541,5808565,6233661,6611261,6705776,7225107,7602305,8074256,8358485,8405758,8452489,8499753,9161299,9350284,9350285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|0.2542097290135062 |\n",
      "|0.0  |(9444472,[282595,424288,706928,894906,1320318,1557022,1557024,1604081,2264580,2264581,2406696,2500892,2547651,2736370,3113587,3160924,3255788,3538395,3821449,4010909,4153235,4200705,4814588,4909212,5146054,5524541,5808565,6091432,6233661,6611261,6705776,7602305,8074256,8358485,8405758,8452489,8499753,9350284,9350285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|0.25418420627624416|\n",
      "|0.0  |(9444472,[235571,282595,424288,612985,659969,706928,1132726,1320318,1557022,1557024,1651141,2264580,2406696,2547651,2736370,3113587,3255788,3585421,4010909,4153235,4200705,4814588,4909212,5146054,5524541,5808565,5808566,6233661,6233662,6611261,6705776,7602305,8074256,8358485,8405758,8452489,8499753,9350284,9350285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])  |0.25392471262687916|\n",
      "|0.0  |(9444472,[282595,424288,471471,706928,1131695,1320318,1557022,1651142,2264580,2406696,2594745,2736370,3113587,3255788,3585421,4010909,4153235,4200705,4767540,4814588,4909212,5146054,5524541,5808565,5808567,6233661,6280595,6611261,6705776,7602305,8074256,8358485,8405758,8452489,8499753,8688750,9161300,9350284,9350285],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|0.25491741985082433|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_probability_model_basic = lambda df: add_probability(df, lr_model_basic)\n",
    "training_predictions = add_probability_model_basic(ohe_train_df).cache()\n",
    "training_predictions.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------------+\n",
      "|label|            features|                  p|\n",
      "+-----+--------------------+-------------------+\n",
      "|  0.0|(9444472,[282595,...| 0.2538831479776069|\n",
      "|  0.0|(9444472,[282595,...| 0.2542097290135062|\n",
      "|  0.0|(9444472,[282595,...|0.25418420627624416|\n",
      "|  0.0|(9444472,[235571,...|0.25392471262687916|\n",
      "|  0.0|(9444472,[282595,...|0.25491741985082433|\n",
      "+-----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_predictions.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(df, model, baseline=None):\n",
    "    \"\"\"Calculates the log loss for the data given the model.\n",
    "\n",
    "    Note:\n",
    "        If baseline has a value the probability should be set to baseline before\n",
    "        the log loss is calculated.  Otherwise, use add_probability to add the\n",
    "        appropriate probabilities to the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame with 'label' and 'features' columns): A DataFrame containing\n",
    "            labels and features.\n",
    "        model (LogisticRegressionModel): A trained logistic regression model. This\n",
    "            can be None if baseline is set.\n",
    "        baseline (float): A baseline probability to use for the log loss calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: Log loss for the data.\n",
    "    \"\"\"\n",
    "    with_probability_df = add_probability(df, model)\n",
    "    with_log_loss_df = add_log_loss(with_probability_df)\n",
    "    log_loss = (with_log_loss_df.select(F.sum('log_loss')).collect()[0][0] / with_log_loss_df.count())\n",
    "    return log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHE Features Train Logloss:\n",
      "\tBaseline = 0.569\n",
      "\tLogReg = 0.567\n"
     ]
    }
   ],
   "source": [
    "log_loss_train_model_basic = evaluate_results(ohe_train_df, lr_model_basic)\n",
    "print ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n",
    "       .format(log_loss_tr_base, log_loss_train_model_basic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Test dataset log loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_val = evaluate_results(ohe_test_df, lr_model_basic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5678805271280248"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ohe_test_df = ohe_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------------------+\n",
      "|label|            features|                  p|\n",
      "+-----+--------------------+-------------------+\n",
      "|  0.0|(9444472,[282595,...|0.25444869744569376|\n",
      "|  0.0|(9444472,[235572,...| 0.2528937973315496|\n",
      "|  0.0|(9444472,[47175,1...|0.25448684505540875|\n",
      "|  0.0|(9444472,[94556,4...|0.25360390349183876|\n",
      "|  0.0|(9444472,[424288,...| 0.2546179123892238|\n",
      "+-----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = add_probability_model_basic(ohe_test_df).cache()\n",
    "test_predictions.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|Probability|  count|\n",
      "+-----------+-------+\n",
      "|       0.26|3378245|\n",
      "|       0.25|1207290|\n",
      "|       0.27|    190|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions.withColumn(\"Probability\",F.round(col(\"p\"),2) ).groupBy(\"Probability\").agg(F.count(lit(1)).alias(\"count\")).show()\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "#one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(ohe_train_df)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "# evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}