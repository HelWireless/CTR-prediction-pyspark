{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import display, HTML, display_html #usefull to display wide tables\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, pandas_histogram\n",
    "from pyspark.sql import functions as F, types, SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toy_rawDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a1596f4f39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoy_rawDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'toy_rawDF' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parse_point(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    values = point.split(',')[1:]\n",
    "    #values = filter(None, values)\n",
    "    indices = range(len(values))\n",
    "    return zip(indices,values)\n",
    "\n",
    "print(list(parse_point(toy_rawDF.select('text').first()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, split\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType, FloatType, DoubleType\n",
    "\n",
    "\n",
    "parse_point_udf = udf(parse_point, ArrayType(StructType([StructField('_1', LongType()),StructField('_2', StringType())])))\n",
    "\n",
    "def parse_raw_df(raw_df):\n",
    "    \"\"\"Convert a DataFrame consisting of rows of comma separated text into labels and feature.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        raw_df (DataFrame with a 'text' column): DataFrame containing the raw comma separated data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with 'label' and 'feature' columns.   \n",
    "  \n",
    "    \"\"\"\n",
    "    return (raw_df.select(split(raw_df.text,',').getItem(0).cast(\"double\").alias('label'),\n",
    "                         parse_point_udf(raw_df.text).alias('features'))\n",
    "                        .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_dict(input_df):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        input_df (DataFrame with 'features' column): A DataFrame where each row contains a list of\n",
    "            (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    input_distinct_feats_df = input_df.select(explode(input_df.features)).distinct()\n",
    "    input_ohe_dict = (input_distinct_feats_df\n",
    "                     .rdd\n",
    "                     .map(lambda r: tuple(r[0]))\n",
    "                     .zipWithIndex().collectAsMap())\n",
    "    return input_ohe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import VectorUDT\n",
    "\n",
    "def ohe_udf_generator(ohe_dict_broadcast):\n",
    "    \"\"\"Generate a UDF that is setup to one-hot-encode rows with the given dictionary.\n",
    "\n",
    "    Note:\n",
    "        We'll reuse this function to generate a UDF that can one-hot-encode rows based on a\n",
    "        one-hot-encoding dictionary built from the training data.  Also, you should calculate\n",
    "        the number of features before calling the one_hot_encoding function.\n",
    "\n",
    "    Args:\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "\n",
    "    Returns:\n",
    "        UserDefinedFunction: A UDF can be used in `DataFrame` `select` statement to call a\n",
    "            function on each row in a given column.  This UDF should call the one_hot_encoding\n",
    "            function with the appropriate parameters.\n",
    "    \"\"\"\n",
    "    length = len(ohe_dict_broadcast.value)\n",
    "    return udf(lambda x: one_hot_encoding(x, ohe_dict_broadcast, length), VectorUDT())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "def one_hot_encoding(raw_feats, ohe_dict_broadcast, num_ohe_feats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        You should ensure that the indices used to create a SparseVector are sorted.\n",
    "\n",
    "    Args:\n",
    "        raw_feats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sample_one)\n",
    "        ohe_dict_broadcast (Broadcast of dict): Broadcast variable containing a dict that maps\n",
    "            (featureID, value) to unique integer.\n",
    "        num_ohe_feats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length num_ohe_feats with indices equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    indices = sorted([ohe_dict_broadcast.value[feat] for feat in raw_feats])\n",
    "    values = np.ones(len(raw_feats))\n",
    "    return SparseVector(num_ohe_feats,indices,values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rawDF = sqlContext.read.text('Toy_Example_Data.csv').withColumnRenamed(\"value\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|1,4,6.37,2.85,0,A...|\n",
      "|1,5,7.84,3.91,1,S...|\n",
      "|1,2,5.5,2.82,1,St...|\n",
      "|1,3,8.43,1.92,1,E...|\n",
      "|0,4,6.29,3.43,1,F...|\n",
      "|1,4,8.36,1.91,1,J...|\n",
      "|1,4,6.56,3.61,1,B...|\n",
      "|0,2,7.98,1.96,0,F...|\n",
      "|1,4,5.52,2.85,0,C...|\n",
      "|1,3,9.15,3.34,0,F...|\n",
      "|1,4,6.14,1.82,1,M...|\n",
      "|0,4,6.87,1.75,1,J...|\n",
      "|1,3,6.9,3.84,1,Sp...|\n",
      "|1,2,6.53,1.57,0,A...|\n",
      "|1,4,9.98,3.66,1,S...|\n",
      "|0,2,9.78,2.81,1,M...|\n",
      "|0,2,5.1,3.32,1,St...|\n",
      "|1,3,5.32,3.62,1,A...|\n",
      "|1,2,7.16,3.72,1,C...|\n",
      "|0,3,6.07,2.29,0,R...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toy_rawDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806 94 100 1000\n"
     ]
    }
   ],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "# Use randomSplit with weights and seed\n",
    "raw_train_df, raw_validation_df, raw_test_df = toy_rawDF.randomSplit(weights, seed)\n",
    "\n",
    "# Cache and count the DataFrames\n",
    "n_train = raw_train_df.cache().count()\n",
    "n_val = raw_validation_df.cache().count()\n",
    "n_test = raw_test_df.cache().count()\n",
    "print(n_train, n_val, n_test, str(n_train + n_val + n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=0.0, features=[Row(_1=0, _2='2'), Row(_1=1, _2='5.01'), Row(_1=2, _2='2.41'), Row(_1=3, _2='1'), Row(_1=4, _2='Math'), Row(_1=5, _2='German')])\n"
     ]
    }
   ],
   "source": [
    "parsed_train_df = parse_raw_df(raw_train_df)\n",
    "print(parsed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (explode, col)\n",
    "num_categories = (parsed_train_df\n",
    "                    .select(explode('features').alias('features'))\n",
    "                    .distinct()\n",
    "                    .select(col('features').getField('_1').alias('featureNumber'))\n",
    "                    .groupBy('featureNumber')\n",
    "                    .sum()\n",
    "                    .orderBy('featureNumber')\n",
    "                    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(featureNumber=0, sum(featureNumber)=0), Row(featureNumber=1, sum(featureNumber)=402), Row(featureNumber=2, sum(featureNumber)=480), Row(featureNumber=3, sum(featureNumber)=6), Row(featureNumber=4, sum(featureNumber)=68), Row(featureNumber=5, sum(featureNumber)=85)]\n"
     ]
    }
   ],
   "source": [
    "print(num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(2, '1.55'): 0,\n",
       " (1, '7.37'): 1,\n",
       " (1, '5.54'): 2,\n",
       " (2, '3.79'): 3,\n",
       " (1, '9.66'): 4,\n",
       " (2, '3.62'): 5,\n",
       " (1, '7.38'): 6,\n",
       " (1, '9.29'): 7,\n",
       " (2, '2.04'): 8,\n",
       " (1, '6'): 9,\n",
       " (2, '3.45'): 10,\n",
       " (2, '3.22'): 11,\n",
       " (2, '3.54'): 12,\n",
       " (1, '6.46'): 13,\n",
       " (1, '7.25'): 14,\n",
       " (1, '7.1'): 15,\n",
       " (1, '9.07'): 16,\n",
       " (2, '3.93'): 17,\n",
       " (1, '6.06'): 18,\n",
       " (1, '8.48'): 19,\n",
       " (2, '1.96'): 20,\n",
       " (2, '1.88'): 21,\n",
       " (1, '6.98'): 22,\n",
       " (5, 'Fine Arts'): 23,\n",
       " (1, '6.82'): 24,\n",
       " (1, '5.56'): 25,\n",
       " (1, '6.6'): 26,\n",
       " (1, '9.37'): 27,\n",
       " (2, '3.86'): 28,\n",
       " (2, '1.98'): 29,\n",
       " (1, '5.97'): 30,\n",
       " (1, '5.73'): 31,\n",
       " (4, 'Electrical Engineering'): 32,\n",
       " (1, '7.16'): 33,\n",
       " (1, '7.44'): 34,\n",
       " (1, '6.43'): 35,\n",
       " (2, '1.93'): 36,\n",
       " (2, '3.13'): 37,\n",
       " (1, '8.26'): 38,\n",
       " (1, '9'): 39,\n",
       " (1, '6.77'): 40,\n",
       " (1, '9.08'): 41,\n",
       " (2, '2.72'): 42,\n",
       " (1, '7.24'): 43,\n",
       " (1, '9.46'): 44,\n",
       " (1, '9.63'): 45,\n",
       " (2, '2.66'): 46,\n",
       " (1, '9.61'): 47,\n",
       " (2, '2.52'): 48,\n",
       " (1, '5.96'): 49,\n",
       " (1, '7.05'): 50,\n",
       " (1, '8.45'): 51,\n",
       " (1, '7.92'): 52,\n",
       " (1, '8.32'): 53,\n",
       " (1, '9.43'): 54,\n",
       " (2, '3.35'): 55,\n",
       " (1, '9.71'): 56,\n",
       " (1, '9.82'): 57,\n",
       " (1, '5.6'): 58,\n",
       " (2, '2.29'): 59,\n",
       " (2, '1.94'): 60,\n",
       " (2, '1.97'): 61,\n",
       " (2, '2.75'): 62,\n",
       " (1, '7.39'): 63,\n",
       " (1, '9.26'): 64,\n",
       " (1, '5.65'): 65,\n",
       " (1, '8.56'): 66,\n",
       " (4, 'Sports Management'): 67,\n",
       " (2, '1.89'): 68,\n",
       " (1, '8.47'): 69,\n",
       " (4, 'Biology'): 70,\n",
       " (1, '5.13'): 71,\n",
       " (1, '7.04'): 72,\n",
       " (1, '7.96'): 73,\n",
       " (1, '9.03'): 74,\n",
       " (1, '7.4'): 75,\n",
       " (1, '7.53'): 76,\n",
       " (2, '2.85'): 77,\n",
       " (1, '5.28'): 78,\n",
       " (2, '1.85'): 79,\n",
       " (1, '6.22'): 80,\n",
       " (1, '8.63'): 81,\n",
       " (3, '1'): 82,\n",
       " (2, '2.25'): 83,\n",
       " (1, '6.74'): 84,\n",
       " (1, '8.22'): 85,\n",
       " (1, '10'): 86,\n",
       " (1, '7.01'): 87,\n",
       " (1, '7.09'): 88,\n",
       " (1, '8.67'): 89,\n",
       " (2, '2.27'): 90,\n",
       " (1, '9.89'): 91,\n",
       " (1, '7.97'): 92,\n",
       " (1, '9.31'): 93,\n",
       " (1, '9.73'): 94,\n",
       " (2, '1.62'): 95,\n",
       " (1, '5.88'): 96,\n",
       " (1, '8.72'): 97,\n",
       " (1, '9.6'): 98,\n",
       " (2, '3.12'): 99,\n",
       " (1, '6.99'): 100,\n",
       " (1, '7.7'): 101,\n",
       " (1, '7.8'): 102,\n",
       " (1, '9.36'): 103,\n",
       " (1, '6.9'): 104,\n",
       " (1, '8.75'): 105,\n",
       " (1, '5.86'): 106,\n",
       " (2, '1.77'): 107,\n",
       " (2, '3.66'): 108,\n",
       " (2, '3.23'): 109,\n",
       " (1, '7.18'): 110,\n",
       " (2, '2.24'): 111,\n",
       " (1, '9.38'): 112,\n",
       " (2, '2.41'): 113,\n",
       " (1, '6.68'): 114,\n",
       " (1, '5.58'): 115,\n",
       " (1, '5.19'): 116,\n",
       " (4, 'Math'): 117,\n",
       " (1, '8.33'): 118,\n",
       " (1, '6.69'): 119,\n",
       " (1, '8.43'): 120,\n",
       " (1, '9.69'): 121,\n",
       " (2, '1.82'): 122,\n",
       " (1, '8.8'): 123,\n",
       " (2, '2.68'): 124,\n",
       " (2, '1.9'): 125,\n",
       " (1, '6.61'): 126,\n",
       " (1, '6.36'): 127,\n",
       " (5, 'Electrical Engineering'): 128,\n",
       " (1, '9.23'): 129,\n",
       " (1, '5.4'): 130,\n",
       " (2, '1.78'): 131,\n",
       " (1, '7.34'): 132,\n",
       " (1, '7.94'): 133,\n",
       " (1, '8.25'): 134,\n",
       " (2, '3.88'): 135,\n",
       " (1, '5.45'): 136,\n",
       " (5, 'Biology'): 137,\n",
       " (2, '3.46'): 138,\n",
       " (1, '5.78'): 139,\n",
       " (2, '2.22'): 140,\n",
       " (1, '6.31'): 141,\n",
       " (1, '7.35'): 142,\n",
       " (1, '6.25'): 143,\n",
       " (1, '6.83'): 144,\n",
       " (1, '6.3'): 145,\n",
       " (2, '2.78'): 146,\n",
       " (1, '7.59'): 147,\n",
       " (1, '7.29'): 148,\n",
       " (1, '6.16'): 149,\n",
       " (1, '7.63'): 150,\n",
       " (1, '5.8'): 151,\n",
       " (5, 'Journalism'): 152,\n",
       " (4, 'Fine Arts'): 153,\n",
       " (2, '3.51'): 154,\n",
       " (2, '1.54'): 155,\n",
       " (1, '5.11'): 156,\n",
       " (1, '6.07'): 157,\n",
       " (1, '9.85'): 158,\n",
       " (1, '6.18'): 159,\n",
       " (1, '6.03'): 160,\n",
       " (2, '2.5'): 161,\n",
       " (1, '7.79'): 162,\n",
       " (2, '3.24'): 163,\n",
       " (1, '6.53'): 164,\n",
       " (2, '2.82'): 165,\n",
       " (2, '1.61'): 166,\n",
       " (1, '8.15'): 167,\n",
       " (2, '2.49'): 168,\n",
       " (1, '8.74'): 169,\n",
       " (1, '8.59'): 170,\n",
       " (1, '9.41'): 171,\n",
       " (5, 'English'): 172,\n",
       " (2, '3.14'): 173,\n",
       " (2, '2.94'): 174,\n",
       " (1, '5.75'): 175,\n",
       " (2, '1.8'): 176,\n",
       " (1, '9.56'): 177,\n",
       " (1, '5.51'): 178,\n",
       " (1, '9.14'): 179,\n",
       " (2, '3.28'): 180,\n",
       " (2, '2.59'): 181,\n",
       " (4, 'Data Science'): 182,\n",
       " (1, '6.88'): 183,\n",
       " (2, '2.64'): 184,\n",
       " (2, '3.26'): 185,\n",
       " (1, '8.7'): 186,\n",
       " (1, '8.81'): 187,\n",
       " (1, '6.64'): 188,\n",
       " (2, '3.32'): 189,\n",
       " (1, '5.5'): 190,\n",
       " (1, '9.84'): 191,\n",
       " (2, '2.84'): 192,\n",
       " (2, '2.33'): 193,\n",
       " (2, '3.37'): 194,\n",
       " (1, '9.99'): 195,\n",
       " (2, '3.6'): 196,\n",
       " (1, '6.33'): 197,\n",
       " (4, 'French'): 198,\n",
       " (2, '2.07'): 199,\n",
       " (1, '9.96'): 200,\n",
       " (2, '3.44'): 201,\n",
       " (2, '3.49'): 202,\n",
       " (2, '3.87'): 203,\n",
       " (1, '6.86'): 204,\n",
       " (1, '7.67'): 205,\n",
       " (2, '2.16'): 206,\n",
       " (2, '3.95'): 207,\n",
       " (1, '9.53'): 208,\n",
       " (1, '6.02'): 209,\n",
       " (2, '3.48'): 210,\n",
       " (1, '6.05'): 211,\n",
       " (2, '1.76'): 212,\n",
       " (1, '6.27'): 213,\n",
       " (2, '1.86'): 214,\n",
       " (1, '7.31'): 215,\n",
       " (1, '9.79'): 216,\n",
       " (1, '5.67'): 217,\n",
       " (1, '9.78'): 218,\n",
       " (1, '7.27'): 219,\n",
       " (1, '8.88'): 220,\n",
       " (1, '6.72'): 221,\n",
       " (1, '8.78'): 222,\n",
       " (1, '8.77'): 223,\n",
       " (1, '5.07'): 224,\n",
       " (1, '6.7'): 225,\n",
       " (2, '3.03'): 226,\n",
       " (2, '2.14'): 227,\n",
       " (2, '2.93'): 228,\n",
       " (1, '9.65'): 229,\n",
       " (5, 'Sports Management'): 230,\n",
       " (1, '5.14'): 231,\n",
       " (2, '3.55'): 232,\n",
       " (1, '5.27'): 233,\n",
       " (1, '5.29'): 234,\n",
       " (4, 'German'): 235,\n",
       " (2, '3.27'): 236,\n",
       " (1, '9.34'): 237,\n",
       " (1, '7.43'): 238,\n",
       " (2, '1.7'): 239,\n",
       " (1, '8.49'): 240,\n",
       " (2, '3.15'): 241,\n",
       " (2, '3.98'): 242,\n",
       " (1, '6.23'): 243,\n",
       " (1, '8.02'): 244,\n",
       " (1, '6.73'): 245,\n",
       " (1, '7.21'): 246,\n",
       " (1, '6.62'): 247,\n",
       " (1, '8.93'): 248,\n",
       " (2, '1.6'): 249,\n",
       " (1, '6.51'): 250,\n",
       " (1, '6.94'): 251,\n",
       " (0, '4'): 252,\n",
       " (2, '2.63'): 253,\n",
       " (1, '5.69'): 254,\n",
       " (1, '7.82'): 255,\n",
       " (2, '2.45'): 256,\n",
       " (2, '2.95'): 257,\n",
       " (1, '8.85'): 258,\n",
       " (2, '2.1'): 259,\n",
       " (1, '5.53'): 260,\n",
       " (1, '9.25'): 261,\n",
       " (1, '9.35'): 262,\n",
       " (1, '8.08'): 263,\n",
       " (2, '3.94'): 264,\n",
       " (2, '3.02'): 265,\n",
       " (2, '3.64'): 266,\n",
       " (2, '1.92'): 267,\n",
       " (1, '8.76'): 268,\n",
       " (1, '7.12'): 269,\n",
       " (1, '9.95'): 270,\n",
       " (2, '2.71'): 271,\n",
       " (2, '1.63'): 272,\n",
       " (4, 'Journalism'): 273,\n",
       " (2, '1.66'): 274,\n",
       " (1, '9.91'): 275,\n",
       " (1, '6.93'): 276,\n",
       " (1, '7.03'): 277,\n",
       " (1, '5.83'): 278,\n",
       " (1, '7.98'): 279,\n",
       " (1, '9.98'): 280,\n",
       " (1, '6.15'): 281,\n",
       " (2, '3.31'): 282,\n",
       " (2, '3.9'): 283,\n",
       " (2, '3.07'): 284,\n",
       " (1, '6.34'): 285,\n",
       " (2, '2.39'): 286,\n",
       " (2, '3.06'): 287,\n",
       " (1, '6.79'): 288,\n",
       " (1, '8.6'): 289,\n",
       " (5, 'Data Science'): 290,\n",
       " (5, 'Religious Studies'): 291,\n",
       " (2, '2.21'): 292,\n",
       " (1, '8.23'): 293,\n",
       " (2, '2.92'): 294,\n",
       " (1, '6.55'): 295,\n",
       " (1, '7.22'): 296,\n",
       " (1, '8.27'): 297,\n",
       " (1, '5.81'): 298,\n",
       " (2, '1.91'): 299,\n",
       " (1, '5.66'): 300,\n",
       " (2, '3.52'): 301,\n",
       " (2, '2.4'): 302,\n",
       " (1, '5.1'): 303,\n",
       " (2, '2.42'): 304,\n",
       " (2, '2.7'): 305,\n",
       " (2, '3.1'): 306,\n",
       " (1, '9.5'): 307,\n",
       " (2, '2.36'): 308,\n",
       " (2, '2.74'): 309,\n",
       " (2, '2.57'): 310,\n",
       " (1, '6.67'): 311,\n",
       " (2, '2.54'): 312,\n",
       " (2, '1.75'): 313,\n",
       " (2, '2.17'): 314,\n",
       " (2, '3.11'): 315,\n",
       " (2, '3.16'): 316,\n",
       " (1, '7.11'): 317,\n",
       " (5, 'French'): 318,\n",
       " (1, '8.79'): 319,\n",
       " (1, '9.72'): 320,\n",
       " (1, '5.57'): 321,\n",
       " (1, '5.25'): 322,\n",
       " (2, '3.01'): 323,\n",
       " (1, '7.61'): 324,\n",
       " (1, '9.81'): 325,\n",
       " (2, '3.59'): 326,\n",
       " (1, '7.42'): 327,\n",
       " (1, '6.5'): 328,\n",
       " (1, '9.2'): 329,\n",
       " (1, '6.8'): 330,\n",
       " (2, '1.84'): 331,\n",
       " (2, '2.67'): 332,\n",
       " (0, '5'): 333,\n",
       " (2, '2.34'): 334,\n",
       " (1, '6.4'): 335,\n",
       " (1, '7.49'): 336,\n",
       " (2, '3.56'): 337,\n",
       " (2, '3.5'): 338,\n",
       " (1, '9.01'): 339,\n",
       " (1, '5.47'): 340,\n",
       " (1, '6.41'): 341,\n",
       " (1, '6.96'): 342,\n",
       " (2, '3.61'): 343,\n",
       " (1, '6.92'): 344,\n",
       " (2, '3.53'): 345,\n",
       " (1, '7.81'): 346,\n",
       " (1, '8.07'): 347,\n",
       " (5, 'Chemistry'): 348,\n",
       " (2, '3.77'): 349,\n",
       " (1, '8.52'): 350,\n",
       " (1, '8.94'): 351,\n",
       " (1, '7.2'): 352,\n",
       " (1, '7.99'): 353,\n",
       " (2, '3.89'): 354,\n",
       " (1, '5.31'): 355,\n",
       " (2, '2.18'): 356,\n",
       " (1, '5.64'): 357,\n",
       " (1, '9.64'): 358,\n",
       " (1, '8.04'): 359,\n",
       " (2, '2.97'): 360,\n",
       " (2, '3.83'): 361,\n",
       " (1, '9.62'): 362,\n",
       " (1, '7.58'): 363,\n",
       " (1, '9.77'): 364,\n",
       " (2, '3.08'): 365,\n",
       " (2, '1.58'): 366,\n",
       " (2, '3.65'): 367,\n",
       " (1, '6.39'): 368,\n",
       " (1, '7.08'): 369,\n",
       " (2, '1.64'): 370,\n",
       " (1, '5.94'): 371,\n",
       " (1, '6.13'): 372,\n",
       " (1, '5.09'): 373,\n",
       " (1, '8.46'): 374,\n",
       " (1, '9.04'): 375,\n",
       " (2, '2.06'): 376,\n",
       " (2, '2.32'): 377,\n",
       " (3, '0'): 378,\n",
       " (1, '7.17'): 379,\n",
       " (2, '2.3'): 380,\n",
       " (1, '8.44'): 381,\n",
       " (1, '6.12'): 382,\n",
       " (2, '2.19'): 383,\n",
       " (1, '5'): 384,\n",
       " (1, '9.24'): 385,\n",
       " (2, '3.73'): 386,\n",
       " (1, '5.21'): 387,\n",
       " (1, '9.92'): 388,\n",
       " (1, '5.89'): 389,\n",
       " (4, 'Religious Studies'): 390,\n",
       " (2, '3.67'): 391,\n",
       " (2, '1.72'): 392,\n",
       " (1, '6.87'): 393,\n",
       " (1, '9.17'): 394,\n",
       " (1, '8.36'): 395,\n",
       " (1, '7.46'): 396,\n",
       " (1, '6.81'): 397,\n",
       " (1, '7.71'): 398,\n",
       " (2, '3.85'): 399,\n",
       " (1, '5.48'): 400,\n",
       " (1, '5.24'): 401,\n",
       " (4, 'Statistics'): 402,\n",
       " (1, '7.13'): 403,\n",
       " (1, '8.24'): 404,\n",
       " (1, '7.91'): 405,\n",
       " (1, '9.3'): 406,\n",
       " (1, '6.76'): 407,\n",
       " (2, '2.77'): 408,\n",
       " (2, '2.86'): 409,\n",
       " (1, '6.28'): 410,\n",
       " (1, '8.54'): 411,\n",
       " (1, '9.05'): 412,\n",
       " (1, '5.01'): 413,\n",
       " (1, '5.62'): 414,\n",
       " (1, '7.95'): 415,\n",
       " (1, '9.86'): 416,\n",
       " (1, '8.03'): 417,\n",
       " (5, 'Spanish'): 418,\n",
       " (1, '6.26'): 419,\n",
       " (2, '1.79'): 420,\n",
       " (1, '8.21'): 421,\n",
       " (1, '8.87'): 422,\n",
       " (1, '6.38'): 423,\n",
       " (1, '7.41'): 424,\n",
       " (1, '7.02'): 425,\n",
       " (1, '8.82'): 426,\n",
       " (1, '6.01'): 427,\n",
       " (1, '5.9'): 428,\n",
       " (1, '6.2'): 429,\n",
       " (2, '2.01'): 430,\n",
       " (2, '1.95'): 431,\n",
       " (1, '9.87'): 432,\n",
       " (2, '1.53'): 433,\n",
       " (2, '2.23'): 434,\n",
       " (1, '9.55'): 435,\n",
       " (2, '2.2'): 436,\n",
       " (2, '2.9'): 437,\n",
       " (2, '3.91'): 438,\n",
       " (2, '2.61'): 439,\n",
       " (2, '2.26'): 440,\n",
       " (2, '1.59'): 441,\n",
       " (2, '3.39'): 442,\n",
       " (2, '3.84'): 443,\n",
       " (1, '8.31'): 444,\n",
       " (2, '2.89'): 445,\n",
       " (1, '9.93'): 446,\n",
       " (4, 'Spanish'): 447,\n",
       " (2, '2.79'): 448,\n",
       " (1, '7.85'): 449,\n",
       " (1, '6.14'): 450,\n",
       " (1, '8.35'): 451,\n",
       " (2, '2.6'): 452,\n",
       " (1, '7.84'): 453,\n",
       " (2, '3.97'): 454,\n",
       " (1, '9.11'): 455,\n",
       " (1, '6.29'): 456,\n",
       " (2, '2.56'): 457,\n",
       " (2, '1.56'): 458,\n",
       " (2, '1.51'): 459,\n",
       " (2, '2.87'): 460,\n",
       " (1, '7.78'): 461,\n",
       " (5, 'Astrophysics'): 462,\n",
       " (1, '5.26'): 463,\n",
       " (1, '7.83'): 464,\n",
       " (2, '2.91'): 465,\n",
       " (1, '9.09'): 466,\n",
       " (1, '5.61'): 467,\n",
       " (1, '7.5'): 468,\n",
       " (1, '5.76'): 469,\n",
       " (2, '3.78'): 470,\n",
       " (1, '8.68'): 471,\n",
       " (2, '2.88'): 472,\n",
       " (1, '9.7'): 473,\n",
       " (2, '3.36'): 474,\n",
       " (1, '7.76'): 475,\n",
       " (1, '5.18'): 476,\n",
       " (0, '3'): 477,\n",
       " (2, '3.09'): 478,\n",
       " (2, '3.04'): 479,\n",
       " (1, '5.2'): 480,\n",
       " (1, '6.19'): 481,\n",
       " (2, '3.76'): 482,\n",
       " (1, '5.52'): 483,\n",
       " (2, '2.48'): 484,\n",
       " (1, '6.59'): 485,\n",
       " (2, '1.69'): 486,\n",
       " (2, '2.96'): 487,\n",
       " (2, '2.12'): 488,\n",
       " (2, '3.05'): 489,\n",
       " (1, '7.26'): 490,\n",
       " (2, '3.21'): 491,\n",
       " (1, '6.17'): 492,\n",
       " (2, '1.67'): 493,\n",
       " (1, '5.72'): 494,\n",
       " (1, '8.98'): 495,\n",
       " (2, '3.41'): 496,\n",
       " (1, '8.9'): 497,\n",
       " (1, '9.1'): 498,\n",
       " (2, '2.51'): 499,\n",
       " (2, '3.38'): 500,\n",
       " (1, '9.58'): 501,\n",
       " (5, 'German'): 502,\n",
       " (2, '2.44'): 503,\n",
       " (2, '3.33'): 504,\n",
       " (1, '7.66'): 505,\n",
       " (1, '7.06'): 506,\n",
       " (1, '5.17'): 507,\n",
       " (1, '8.83'): 508,\n",
       " (1, '9.22'): 509,\n",
       " (2, '3.69'): 510,\n",
       " (2, '1.65'): 511,\n",
       " (2, '3.82'): 512,\n",
       " (1, '8.91'): 513,\n",
       " (1, '7.55'): 514,\n",
       " (1, '9.02'): 515,\n",
       " (1, '9.59'): 516,\n",
       " (2, '3.75'): 517,\n",
       " (1, '9.06'): 518,\n",
       " (1, '5.44'): 519,\n",
       " (1, '8.05'): 520,\n",
       " (1, '5.34'): 521,\n",
       " (4, 'Astronomy'): 522,\n",
       " (1, '5.04'): 523,\n",
       " (2, '2.28'): 524,\n",
       " (1, '8.12'): 525,\n",
       " (1, '6.84'): 526,\n",
       " (5, 'Math'): 527,\n",
       " (1, '6.75'): 528,\n",
       " (2, '3.8'): 529,\n",
       " (1, '8.01'): 530,\n",
       " (2, '2.05'): 531,\n",
       " (2, '1.73'): 532,\n",
       " (1, '8.71'): 533,\n",
       " (1, '6.47'): 534,\n",
       " (1, '9.28'): 535,\n",
       " (5, 'Astronomy'): 536,\n",
       " (1, '9.13'): 537,\n",
       " (1, '6.63'): 538,\n",
       " (1, '5.91'): 539,\n",
       " (5, 'Computer Science'): 540,\n",
       " (2, '2.03'): 541,\n",
       " (1, '9.52'): 542,\n",
       " (1, '5.92'): 543,\n",
       " (1, '7.74'): 544,\n",
       " (1, '9.18'): 545,\n",
       " (2, '1.52'): 546,\n",
       " (1, '7.75'): 547,\n",
       " (1, '5.42'): 548,\n",
       " (2, '3.74'): 549,\n",
       " (1, '8.66'): 550,\n",
       " (2, '2.99'): 551,\n",
       " (1, '6.08'): 552,\n",
       " (2, '3.43'): 553,\n",
       " (2, '2.38'): 554,\n",
       " (2, '2.83'): 555,\n",
       " (1, '8.86'): 556,\n",
       " (1, '8.55'): 557,\n",
       " (1, '5.35'): 558,\n",
       " (2, '3.63'): 559,\n",
       " (2, '3.57'): 560,\n",
       " (2, '1.57'): 561,\n",
       " (1, '8.92'): 562,\n",
       " (1, '5.02'): 563,\n",
       " (2, '3.3'): 564,\n",
       " (2, '2.46'): 565,\n",
       " (1, '8.96'): 566,\n",
       " (2, '2.53'): 567,\n",
       " (1, '7.3'): 568,\n",
       " (1, '9.16'): 569,\n",
       " (1, '5.84'): 570,\n",
       " (2, '1.83'): 571,\n",
       " (1, '5.46'): 572,\n",
       " (1, '6.21'): 573,\n",
       " (1, '7.62'): 574,\n",
       " (1, '6.45'): 575,\n",
       " (2, '2.76'): 576,\n",
       " (2, '2.55'): 577,\n",
       " (1, '5.79'): 578,\n",
       " (1, '7'): 579,\n",
       " (1, '8.41'): 580,\n",
       " (1, '7.36'): 581,\n",
       " (1, '8.39'): 582,\n",
       " (1, '8.95'): 583,\n",
       " (2, '2.62'): 584,\n",
       " (2, '3.99'): 585,\n",
       " (1, '9.68'): 586,\n",
       " (1, '5.71'): 587,\n",
       " (2, '3.92'): 588,\n",
       " (1, '8'): 589,\n",
       " (1, '5.05'): 590,\n",
       " (1, '9.57'): 591,\n",
       " (1, '6.24'): 592,\n",
       " (2, '3.17'): 593,\n",
       " (2, '1.81'): 594,\n",
       " (2, '2.11'): 595,\n",
       " (1, '7.28'): 596,\n",
       " (4, 'English'): 597,\n",
       " (2, '3.72'): 598,\n",
       " (2, '3.81'): 599,\n",
       " (5, 'Statistics'): 600,\n",
       " (1, '6.91'): 601,\n",
       " (2, '1.5'): 602,\n",
       " (1, '6.54'): 603,\n",
       " (1, '8.14'): 604,\n",
       " (2, '1.68'): 605,\n",
       " (2, '2.69'): 606,\n",
       " (1, '9.74'): 607,\n",
       " (1, '5.82'): 608,\n",
       " (2, '2.15'): 609,\n",
       " (2, '1.87'): 610,\n",
       " (1, '9.8'): 611,\n",
       " (2, '1.74'): 612,\n",
       " (1, '9.97'): 613,\n",
       " (2, '2.31'): 614,\n",
       " (4, 'Astrophysics'): 615,\n",
       " (1, '7.47'): 616,\n",
       " (1, '9.67'): 617,\n",
       " (1, '6.09'): 618,\n",
       " (1, '8.73'): 619,\n",
       " (1, '9.21'): 620,\n",
       " (1, '9.47'): 621,\n",
       " (2, '3'): 622,\n",
       " (1, '6.37'): 623,\n",
       " (4, 'Computer Science'): 624,\n",
       " (1, '6.11'): 625,\n",
       " (1, '8.09'): 626,\n",
       " (1, '9.4'): 627,\n",
       " (2, '2.58'): 628,\n",
       " (1, '9.44'): 629,\n",
       " (2, '2.73'): 630,\n",
       " (1, '9.15'): 631,\n",
       " (1, '8.11'): 632,\n",
       " (1, '5.99'): 633,\n",
       " (2, '2'): 634,\n",
       " (2, '3.18'): 635,\n",
       " (2, '2.8'): 636,\n",
       " (1, '8.4'): 637,\n",
       " (1, '9.45'): 638,\n",
       " (2, '3.42'): 639,\n",
       " (1, '7.88'): 640,\n",
       " (2, '3.7'): 641,\n",
       " (1, '7.51'): 642,\n",
       " (1, '8.53'): 643,\n",
       " (1, '6.66'): 644,\n",
       " (1, '5.23'): 645,\n",
       " (1, '7.87'): 646,\n",
       " (1, '5.87'): 647,\n",
       " (2, '2.98'): 648,\n",
       " (2, '2.43'): 649,\n",
       " (1, '5.06'): 650,\n",
       " (1, '5.38'): 651,\n",
       " (1, '8.58'): 652,\n",
       " (1, '6.04'): 653,\n",
       " (2, '3.29'): 654,\n",
       " (2, '2.35'): 655,\n",
       " (2, '1.99'): 656,\n",
       " (2, '4'): 657,\n",
       " (4, 'Chemistry'): 658,\n",
       " (1, '6.56'): 659,\n",
       " (1, '5.39'): 660,\n",
       " (1, '8.97'): 661,\n",
       " (1, '5.33'): 662,\n",
       " (2, '1.71'): 663,\n",
       " (2, '2.81'): 664,\n",
       " (2, '3.2'): 665,\n",
       " (2, '3.34'): 666,\n",
       " (1, '7.45'): 667,\n",
       " (1, '6.89'): 668,\n",
       " (1, '8.5'): 669,\n",
       " (2, '2.47'): 670,\n",
       " (1, '8.38'): 671,\n",
       " (1, '6.1'): 672,\n",
       " (2, '2.08'): 673,\n",
       " (0, '2'): 674,\n",
       " (1, '6.49'): 675,\n",
       " (1, '6.97'): 676,\n",
       " (1, '7.89'): 677,\n",
       " (1, '5.08'): 678,\n",
       " (2, '3.19'): 679,\n",
       " (1, '5.15'): 680,\n",
       " (2, '3.58'): 681}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr_ohe_dict = create_one_hot_dict(parsed_train_df)\n",
    "num_ctr_ohe_feats = len(ctr_ohe_dict)\n",
    "print(num_ctr_ohe_feats)\n",
    "ctr_ohe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(682,[82,113,117,...|\n",
      "|  0.0|(682,[23,82,185,3...|\n",
      "|  0.0|(682,[70,82,234,2...|\n",
      "|  0.0|(682,[82,355,418,...|\n",
      "|  0.0|(682,[82,462,576,...|\n",
      "|  0.0|(682,[176,378,418...|\n",
      "|  0.0|(682,[260,378,418...|\n",
      "|  0.0|(682,[378,414,447...|\n",
      "|  0.0|(682,[217,378,522...|\n",
      "|  0.0|(682,[235,378,536...|\n",
      "|  0.0|(682,[82,151,152,...|\n",
      "|  0.0|(682,[82,230,272,...|\n",
      "|  0.0|(682,[160,227,378...|\n",
      "|  0.0|(682,[82,153,160,...|\n",
      "|  0.0|(682,[131,182,348...|\n",
      "|  0.0|(682,[67,82,137,5...|\n",
      "|  0.0|(682,[32,137,250,...|\n",
      "|  0.0|(682,[82,137,154,...|\n",
      "|  0.0|(682,[117,267,330...|\n",
      "|  0.0|(682,[82,144,370,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "[Row(label=0.0, features=SparseVector(682, {82: 1.0, 113: 1.0, 117: 1.0, 413: 1.0, 502: 1.0, 674: 1.0}))]\n"
     ]
    }
   ],
   "source": [
    "ohe_dict_broadcast = sc.broadcast(ctr_ohe_dict)\n",
    "ohe_dict_udf =  ohe_udf_generator(ohe_dict_broadcast)\n",
    "ohe_train_df =  parsed_train_df.select(parsed_train_df.label, ohe_dict_udf(parsed_train_df.features).alias('features'))\n",
    "#ohe_train_df.show(1)                  \n",
    "\n",
    "print(ohe_train_df.count())\n",
    "print(ohe_train_df.show())\n",
    "print(ohe_train_df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_rdd = ohe_train_df \\\n",
    "                     .rdd \\\n",
    "                     .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.6625310173697274\n",
      "Variance: 0.2235836683927612\n"
     ]
    }
   ],
   "source": [
    "meanDropOut = ohe_train_rdd.map(lambda x: x[0]).mean()\n",
    "varDropOut = ohe_train_rdd.map(lambda x: x[0]).variance()\n",
    "print(f\"Mean: {meanDropOut}\")\n",
    "print(f\"Variance: {varDropOut}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = np.append(meanDropOut, np.zeros(ohe_train_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.1, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps):  \n",
    "        ############## YOUR CODE HERE #############\n",
    "        model = GDUpdate(trainRDD, model, learningRate)\n",
    "        training_loss = LogLoss(trainRDD, model) \n",
    "        test_loss = LogLoss(testRDD, model)\n",
    "        ############## (END) YOUR CODE #############\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[round(w,3) for w in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorCurves(trainLoss, testLoss, title = None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting.\n",
    "    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize = (16,8))\n",
    "    x = list(range(len(trainLoss)))[1:]\n",
    "    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n",
    "    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n",
    "    ax.legend(loc='upper right', fontsize='x-large')\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Log loss')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute log loss.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    augmentedData = dataRDD.map(lambda x: (x[0], np.append([1.0], x[1])))\n",
    "    ################## YOUR CODE HERE ##################\n",
    "    loss = augmentedData.map(lambda x: (-x[0] * np.log(sigmoid(W.dot(x[1]))) - (1 - x[0]) * np.log(1 - sigmoid(W.dot(x[1])))) ).mean()\n",
    "    ################## (END) YOUR CODE ##################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: ( x[0], np.append([1.0], x[1]))).cache()\n",
    "    \n",
    "    ################## YOUR CODE HERE ################# \n",
    "    grad = augmentedData.map(lambda x: (sigmoid(W.dot(x[1])) - x[0])*x[1]).mean()\n",
    "    new_model = W - learningRate * grad\n",
    "    ################## (END) YOUR CODE ################# \n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 1024, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-27-60a53a15126c>\", line 10, in <lambda>\nValueError: shapes (807,) and (683,) not aligned: 807 (dim 0) != 683 (dim 0)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-27-60a53a15126c>\", line 10, in <lambda>\nValueError: shapes (807,) and (683,) not aligned: 807 (dim 0) != 683 (dim 0)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d1b84a721687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnSteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBASELINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"BASELINE:  Loss = {LogLoss(ohe_train_rdd,model)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-60a53a15126c>\u001b[0m in \u001b[0;36mLogLoss\u001b[0;34m(dataRDD, W)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maugmentedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m################## YOUR CODE HERE ##################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m################## (END) YOUR CODE ##################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \"\"\"\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \"\"\"\n\u001b[1;32m    833\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 1024, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-27-60a53a15126c>\", line 10, in <lambda>\nValueError: shapes (807,) and (683,) not aligned: 807 (dim 0) != 683 (dim 0)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:162)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 2457, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 370, in func\n    return f(iterator)\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1083, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-27-60a53a15126c>\", line 10, in <lambda>\nValueError: shapes (807,) and (683,) not aligned: 807 (dim 0) != 683 (dim 0)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "nSteps = 5\n",
    "model = BASELINE\n",
    "print(f\"BASELINE:  Loss = {LogLoss(ohe_train_rdd,model)}\")\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(ohe_train_rdd, model)\n",
    "    loss = LogLoss(ohe_train_rdd, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
